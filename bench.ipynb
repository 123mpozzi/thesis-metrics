{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrH5gGJWRRLU"
      },
      "source": [
        "# Mount Google Drive and get predictions\n",
        "\n",
        "Modify the code to use it locally.  \n",
        "The code was aimed at producing performance tables featured in the thesis, hence it is very specific and rigid\n",
        "\n",
        "---\n",
        "The paths of the predictions must respect the following:  \n",
        "- Folder structure must be `detector_name/pred_type/pred_folder` and be placed in the `dataset` folder  \n",
        "Where `detector_name` refers to the old names of the approaches and may assume: `skinny`, `bayes` (probabilistic), `dyc` (nbrancati)  \n",
        "Where `pred_type` is either: `base` or `cross`  \n",
        "Where `pred_folder` is the folder containing the actual predictions (within `p`, `x`, `y` subfolders); eg. `ecu` (representing a base prediction), or `hgr_on_schmugge` (representing a cross prediction)  \n",
        "- If evaluating skin tones, append `_st` to `detector_name`\n",
        "- `dyc` must use `hgr_small` instead of simply `hgr` in the base predictions: `dataset/dyc/base/hgr_small`\n",
        "- All path names must be in lowercase\n",
        "\n",
        "Some complete examples are the following:  \n",
        "```\n",
        "dataset/skinny/base/ecu\n",
        "dataset/skinny/base/hgr\n",
        "dataset/skinny/base/schmugge\n",
        "\n",
        "dataset/bayes/cross/ecu_on_hgr\n",
        "dataset/bayes/cross/ecu_on_schmugge\n",
        "dataset/bayes/cross/hgr_on_ecu\n",
        "dataset/bayes/cross/hgr_on_schmugge\n",
        "dataset/bayes/cross/schmugge_on_ecu\n",
        "dataset/bayes/cross/schmugge_on_hgr\n",
        "\n",
        "dataset/dyc_st/base/dark\n",
        "dataset/dyc_st/base/light\n",
        "dataset/dyc_st/base/medium\n",
        "```\n",
        "\n",
        "---\n",
        "The paths of performance dumps must respect the following:\n",
        "- Folder structure must be `bench_detector_name` and be placed in the `performance` folder  \n",
        "Where `detector_name` refers to the old names of the approaches and may assume: `skinny`, `bayes` (probabilistic), `dyc` (nbrancati)  \n",
        "- Dumps must be named `benchN.txt` where N is an integer starting from `0`\n",
        "- All path names must be in lowercase\n",
        "\n",
        "Some complete examples are the following:  \n",
        "```\n",
        "bench_bayes/bench0.txt\n",
        "bench_bayes/bench1.txt\n",
        "..\n",
        "bench_bayes/bench4.txt\n",
        "\n",
        "bench_skinny/bench0.txt\n",
        "bench_dyc/bench0.txt\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjczG0hjRRWD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz9PBhYUNs0U"
      },
      "outputs": [],
      "source": [
        "bench_mode = 'dataset' # 'dataset' or 'performance'\n",
        "\n",
        "if bench_mode == 'dataset':\n",
        "    !rm -rf dataset\n",
        "\n",
        "    !unzip drive/MyDrive/testing/skinny/20210521-204405_p.zip -d dataset # skinny full\n",
        "    !mv dataset/20210521-204405/* dataset/skinny\n",
        "    !unzip drive/MyDrive/testing/skinny/20210523-192002_p.zip -d dataset # skinny skintones full\n",
        "    !mv dataset/20210523-192002/* dataset/skinny_st\n",
        "\n",
        "    !unzip drive/MyDrive/testing/bayes/bayes.zip -d dataset # probabilistic full\n",
        "    !unzip drive/MyDrive/testing/bayes/bayes_st.zip -d dataset # probabilistic skintones full\n",
        "\n",
        "    !unzip drive/MyDrive/testing/dyc/dyc.zip -d dataset # nbrancati\n",
        "    !unzip drive/MyDrive/testing/dyc/dyc_st.zip -d dataset # nbrancati\n",
        "elif bench_mode == 'performance':\n",
        "    !rm -rf performance\n",
        "\n",
        "    !unzip drive/MyDrive/testing/benchmark/bench_skinny.zip -d performance\n",
        "    !unzip drive/MyDrive/testing/benchmark/bench_bayes.zip -d performance\n",
        "    !unzip drive/MyDrive/testing/benchmark/bench_dyc.zip -d performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1EYV3yPVB4X"
      },
      "source": [
        "# Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CYikmZwTVB4Y"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#  Measure the goodness of the classifier by comparing predictions with groundtruths\n",
        "\n",
        "#  In skin detection, false negatives may weight more as they cannot be fixed by post-processing\n",
        "#  whereas false positives can to a degree\n",
        "\n",
        "\n",
        "# Prevent zero division\n",
        "smooth = 1e-20\n",
        "\n",
        "\n",
        "def confmat_scores(y_true, y_pred) -> dict:\n",
        "    '''\n",
        "    Return a dict that can be used as a LUT-table of the confusion matrix scores\n",
        "    For info on each score, see https://en.wikipedia.org/wiki/Precision_and_recall\n",
        "    '''\n",
        "    data = {}\n",
        "    cast_type = 'double'\n",
        "\n",
        "    neg_y_true = 1 - y_true\n",
        "    neg_y_pred = 1 - y_pred\n",
        "\n",
        "    # dtype casting is used to prevent overflow long_scalars\n",
        "    AP = np.sum(y_true, dtype=cast_type) # TP + FN\n",
        "    AN = np.sum(neg_y_true, dtype='double') # TN + FP\n",
        "    SE = np.sum(y_pred, dtype='double') #TP + FP\n",
        "    TP = np.sum(y_true * y_pred, dtype='double')\n",
        "    FP = SE - TP\n",
        "    TN = np.sum(neg_y_true * neg_y_pred, dtype='double')\n",
        "    FN = AP - TP\n",
        "\n",
        "    data['ap'] = AP\n",
        "    data['an'] = AN\n",
        "    data['se'] = SE\n",
        "    data['tp'] = TP\n",
        "    data['fp'] = FP\n",
        "    data['tn'] = TN\n",
        "    data['fn'] = FN\n",
        "\n",
        "    return data\n",
        "\n",
        "def iou_logical(y_true, y_pred) -> float:\n",
        "    '''Intersection over Union'''\n",
        "    overlap = y_true * y_pred # Logical AND\n",
        "    union =   y_true + y_pred # Logical OR\n",
        "    # Note that matrices are bool due to '> threshold' in load_images(),\n",
        "    # it they were not, for union must to use bitwise OR '|'\n",
        "    \n",
        "    # Treats \"True\" as 1, sums number of Trues\n",
        "    # in overlap and union and divides\n",
        "    IOU = overlap.sum() / (union.sum() + smooth) \n",
        "    return IOU\n",
        "\n",
        "def iou(cs):\n",
        "    '''\n",
        "    Intersection over Union can be re-expressed in terms of precision and recall\n",
        "    Credit to https://tomkwok.com/posts/iou-vs-f1/\n",
        "    '''\n",
        "    return cs['tp'] / (cs['tp'] + cs['fp'] + cs['fn'] + smooth)\n",
        "\n",
        "def recall(cs):\n",
        "    '''\n",
        "    Recall (aliases: TruePositiveRate, Sensitivity)\n",
        "\n",
        "    How many relevant items are selected?\n",
        "    '''\n",
        "    return cs['tp'] / (cs['ap'] + smooth)\n",
        "\n",
        "def specificity(cs):\n",
        "    '''\n",
        "    Specificity (aliases: FalsePositiveRate)\n",
        "\n",
        "    How many negative elements are truly negative?\n",
        "    '''\n",
        "    return cs['tn'] / (cs['an'] + smooth)\n",
        "\n",
        "def precision(cs):\n",
        "    '''How many selected items are relevant?'''\n",
        "    return cs['tp'] / (cs['se'] + smooth)\n",
        "\n",
        "def fb(cs, b = 1):\n",
        "    '''\n",
        "    Fb-measure: recall is considered Beta(b) times important as precision.\n",
        "    For example, F2 weights recall higher than precision, while\n",
        "    F0.5 weights precision higher than recall.\n",
        "    \n",
        "    Beta(b) is a positive real factor\n",
        "    '''\n",
        "    precision_score = precision(cs)\n",
        "    recall_score = recall(cs)\n",
        "    return (1 + b**2) * ((precision_score * recall_score) / ((b**2 * precision_score) + recall_score + smooth))\n",
        "\n",
        "def f1(cs):\n",
        "    '''F1-score (aliases: F1-measure, F-score with Beta=1)'''\n",
        "    return fb(cs)\n",
        "\n",
        "def f2(cs):\n",
        "    '''F2-score'''\n",
        "    return fb(cs, 2)\n",
        "\n",
        "def f1_medium(pr, re, sp):\n",
        "    '''\n",
        "    F1-score (aliases: F1-measure, F-score with Beta=1)\n",
        "    ---\n",
        "    Implementation suited for medium averaging\n",
        "    '''\n",
        "    return 2 * ((pr * re) / (pr + re + smooth))\n",
        "\n",
        "def dprs(cs):\n",
        "    '''\n",
        "    Measures the Euclidean distance between the segmentation,\n",
        "    represented by the point (PR, RE, SP), and the ground truth, the ideal point(1, 1, 1),\n",
        "    hence lower values are better.\n",
        "    Note: it considers all three of Precision, Recall, and Specificity.\n",
        "\n",
        "    Can be higher than 1 in extremely bad cases\n",
        "\n",
        "    ---\n",
        "    Intawong, K., Scuturici, M., & Miguet, S. (2013). A New Pixel-Based Quality Measure\n",
        "    for Segmentation Algorithms Integrating Precision, Recall and Specificity.\n",
        "    Computer Analysis of Images and Patterns, 188-195.\n",
        "    https://doi.org/10.1007/978-3-642-40261-6_22\n",
        "    '''\n",
        "    a = (1 - precision(cs))**2\n",
        "    b = (1 - recall(cs))**2\n",
        "    c = (1 - specificity(cs))**2\n",
        "    \n",
        "    return math.sqrt(a + b + c)\n",
        "\n",
        "def dprs_medium(pr, re, sp):\n",
        "    '''\n",
        "    Measures the Euclidean distance between the segmentation,\n",
        "    represented by the point (PR, RE, SP), and the ground truth, the ideal point(1, 1, 1),\n",
        "    hence lower values are better.\n",
        "    Note: it considers all three of Precision, Recall, and Specificity.\n",
        "\n",
        "    Can be higher than 1 in extremely bad cases\n",
        "\n",
        "    ---\n",
        "    Intawong, K., Scuturici, M., & Miguet, S. (2013). A New Pixel-Based Quality Measure\n",
        "    for Segmentation Algorithms Integrating Precision, Recall and Specificity.\n",
        "    Computer Analysis of Images and Patterns, 188-195.\n",
        "    https://doi.org/10.1007/978-3-642-40261-6_22\n",
        "\n",
        "    ---\n",
        "    Implementation suited for medium averaging\n",
        "    '''\n",
        "    a = (1 - pr)**2\n",
        "    b = (1 - re)**2\n",
        "    c = (1 - sp)**2\n",
        "    return math.sqrt(a + b + c)\n",
        "\n",
        "# Note: the function has not been tested thoroughly and needs to be verified\n",
        "# range is [-1 1]\n",
        "def mcc(cs):\n",
        "    '''\n",
        "    Common statistical measures can dangerously show overoptimistic inflated results,\n",
        "    especially on imbalanced datasets.\n",
        "\n",
        "    The Matthews correlation coefficient (MCC), instead, is a more reliable statistical\n",
        "    rate which produces a high score only if the prediction obtained good results\n",
        "    in all of the four confusion matrix categories (true positives, false negatives,\n",
        "    true negatives, and false positives), proportionally both to the size of positive\n",
        "    elements and the size of negative elements in the dataset.\n",
        "\n",
        "    Range of values is [-1 1]\n",
        "\n",
        "    ---\n",
        "    Chicco, D., & Jurman, G. (2020). The advantages of the Matthews correlation\n",
        "    coefficient (MCC) over F1 score and accuracy in binary classification evaluation.\n",
        "    BMC Genomics, 21(1).\n",
        "    https://doi.org/10.1186/s12864-019-6413-7\n",
        "\n",
        "    ---\n",
        "    Info on F1 vs MCC from the paper analysis is simulated in\n",
        "    `tests/test_metrics.py -> mcc_unittest()`\n",
        "    '''\n",
        "\n",
        "    # The following fixes prevent where MCC could not be calculated normally\n",
        "    M = np.matrix([[cs['tp'], cs['fn']], [cs['fp'], cs['tn']]]) # define confusion matrix\n",
        "    nz = np.count_nonzero(M) # get non-zero elements of the matrix\n",
        "    \n",
        "    # Fix 1\n",
        "    if nz == 1: # 3 elements of M are 0\n",
        "        # all samples of the dataset belong to 1 class\n",
        "        if cs['tp'] != 0 or cs['tn'] != 0: # they either are all correctly classified\n",
        "            return 1\n",
        "        else:\n",
        "            return -1 # or all uncorrectly classified\n",
        "    \n",
        "    # Fix 2\n",
        "    # Where a row or a column of M are zero while the other true entries\n",
        "    # are non zero, MCC takes the indefinite form 0/0\n",
        "    if nz == 2 and np.sum(np.abs(M.diagonal())) != 0 and np.sum(np.abs(np.diag(np.fliplr(M)))) != 0:\n",
        "        # replace the zero elements with an arbitrary small value \n",
        "        M[M == 0] = smooth\n",
        "    \n",
        "    # Calculate MCC\n",
        "    num = cs['tp'] * cs['tn'] - cs['fp'] * cs['fn']\n",
        "    den = math.sqrt((cs['tp'] + cs['fp']) * (cs['tp'] + cs['fn']) * (cs['tn'] + cs['fp']) * (cs['tn'] + cs['fn']))\n",
        "\n",
        "    return num / (den + smooth)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdxRoPQ8VB4d"
      },
      "source": [
        "# Define Metric Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "li6dz5rB29EF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from statistics import mean, pstdev\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def load_images(gt_path: str, pred_path: str, threshold: int = 128):\n",
        "    '''Load images as numpy boolean arrays'''\n",
        "    # Load as grayscale uint8\n",
        "    gt_gray = np.array(Image.open(gt_path).convert('L'))\n",
        "    pred_gray = np.array(Image.open(pred_path).convert('L'))\n",
        "    # Binarize and convert to bool\n",
        "    gt_bool = gt_gray > threshold\n",
        "    pred_bool = pred_gray > threshold\n",
        "    return gt_bool, pred_bool\n",
        "\n",
        "# MEDIUM AVERAGE: calculate average only of medium-scores (PRecision, REcall, SPecificity)\n",
        "# Note: y and p files must have the same filename\n",
        "def calc_metrics(gt_dir: str, pred_dir: str, metric_fns: list, threshold: int = 128) -> list:\n",
        "    '''\n",
        "    Compute all the given metric functions over all images in a folder\n",
        "    by considering a single image at a time and comparing\n",
        "    its groundtruth to its prediction map\n",
        "\n",
        "    Return a list of dicts.\n",
        "    Each dict represents the metrics measurement on a single image\n",
        "\n",
        "    Medium-averaging metric functions get skipped as they cannot be computed on a single image\n",
        "    '''\n",
        "    out = []\n",
        "\n",
        "    # Loop images\n",
        "    for y_filename in tqdm(os.listdir(gt_dir)):\n",
        "        y_path = os.path.join(gt_dir, y_filename)\n",
        "        p_filename = os.path.splitext(y_filename)[0] + '.png'\n",
        "        p_path = os.path.join(pred_dir, p_filename) # pred are always PNG\n",
        "\n",
        "        # Start adding current image data into a dict structure\n",
        "        idata = {}\n",
        "        idata['y'] = y_path\n",
        "        idata['p'] = p_path\n",
        "\n",
        "        # Load images from paths and apply threshold to binarize\n",
        "        # the skin probability maps obtained from predictions\n",
        "        y_true, y_pred = load_images(y_path, p_path, threshold)\n",
        "        # Calculate confusion matrix scores for current image\n",
        "        confmat = confmat_scores(y_true, y_pred)\n",
        "\n",
        "        # Calculate metrics for current image and add them to the dict structure\n",
        "        for metric_fn in metric_fns:\n",
        "            f_name = metric_fn.__name__\n",
        "            f_argcount = metric_fn.__code__.co_argcount # amount of argument in function definition\n",
        "\n",
        "            if f_name.endswith('_medium'): # is a medium-average metric, must not compute now\n",
        "                continue\n",
        "\n",
        "            # only one args: the metric only uses confusion matrix scores and is LUT-optimized\n",
        "            if f_argcount == 1:\n",
        "                idata[f_name] = metric_fn(confmat)\n",
        "            # two args: confusion matrix scores aren't enough\n",
        "            else:\n",
        "                idata[f_name] = metric_fn(y_true, y_pred)\n",
        "        \n",
        "        # Update the final list with current image data\n",
        "        out.append(idata)\n",
        "    \n",
        "    print(f'  Found {len(out)} matches')\n",
        "    return out\n",
        "\n",
        "\n",
        "def calc_mean_metrics(measurements_list: list, metric_fns: list, desc: str, method: str) -> None:\n",
        "    '''\n",
        "    Print human-readable metrics results data\n",
        "    \n",
        "    Process a list of single measurements and\n",
        "    return a dict containing each metric mean and stdev values\n",
        "    \n",
        "    PLEASE NOTE\n",
        "    ---\n",
        "    The mean F1 value calculated by summing all experiments F1 and dividing by N elements\n",
        "    is different than the mean calculated by applying the F1 formula on average REcall and PRecision!\n",
        "\n",
        "    #### Medium Averaging\n",
        "    In the code I call 'medium average' the metrics in which I average only the\n",
        "    medium-scores (PRecision, REcall, SPecificity)\n",
        "    and in the end I calculate the functions of the 'final' metrics (F1, dprs) using these averages\n",
        "\n",
        "    'Medium' as in their formulas they use the basic metrics (the ones in a confusion matrix:\n",
        "    True Positives, False Negatives, ..), while 'final' metrics\n",
        "    use the medium metrics themselves in their formulas.\n",
        "\n",
        "    By following this logic, 'final' averaging means calculating the final metrics\n",
        "    at the first step, along with the medium metrics, for each image and averaging\n",
        "    these values on the batch of images.\n",
        "\n",
        "    In a mathematical way:\n",
        "    f1: 2 * precision * recall / (precision + recall)\n",
        "    f1_finavg: avg(f1)\n",
        "    f1_medavg: 2 * avg(precision) * avg(recall) / (avg(precision) + avg(recall))\n",
        "\n",
        "    '''\n",
        "    print(desc)\n",
        "    res = {}\n",
        "    # Insert datasets and method data into the resulting dict\n",
        "    res['method'] = method\n",
        "    try:\n",
        "        desc = desc.split(' ')[0] # remove hash string\n",
        "        desc = os.path.normpath(desc) # remove trailing slash\n",
        "        desc = os.path.basename(desc) # get prediction folder name\n",
        "        desc = desc.lower().replace('_small', '') # lower case and rename HGR_small to HGR\n",
        "        dss = desc.split('_on_') # split training and predicting datasets\n",
        "        ds_tr = dss[0]\n",
        "        ds_te = dss[1]\n",
        "        res['train'] = ds_tr\n",
        "        res['test'] = ds_te\n",
        "    except: # eg. for testing\n",
        "        pass\n",
        "\n",
        "    medium_avg = []\n",
        "\n",
        "    for metric_fn in metric_fns:\n",
        "        f_name = metric_fn.__name__\n",
        "        f_score = -99\n",
        "\n",
        "        if f_name.endswith('_medium'): # is a medium-average metric\n",
        "            medium_avg.append(metric_fn)\n",
        "            continue\n",
        "        else:\n",
        "            f_data = [ d[f_name] for d in measurements_list ]\n",
        "            f_mean = mean(f_data)\n",
        "            f_mean = '{:.4f}'.format(f_mean) # round to 4 decimals and zerofill\n",
        "            f_std = pstdev(f_data)\n",
        "            f_std = '{:.2f}'.format(f_std) # round to 2 decimals and zerofill\n",
        "            #print(f'{f_name}: {f_mean} ± {f_std}')\n",
        "            # add each metric data to the dict\n",
        "            res[f_name] = f'{f_mean} ± {f_std}'\n",
        "    \n",
        "    # The 'medium average' metrics average only the intermediate-scores (PRecision, REcall, SPecificity)\n",
        "    # and then calculate the functions of the final metrics\n",
        "    for metric_fn in medium_avg:\n",
        "        f_name = metric_fn.__name__\n",
        "        # Calculate the medium-average score using medium-scores averages\n",
        "        pr = float(res['precision'].split(' ')[0])\n",
        "        re = float(res['recall'].split(' ')[0])\n",
        "        sp = float(res['specificity'].split(' ')[0])\n",
        "        f_score = metric_fn(pr, re, sp)\n",
        "        f_score = '{:.4f}'.format(f_score)\n",
        "        res[f_name] = f_score\n",
        "    \n",
        "    for key, value in sorted(res.items()):\n",
        "        print(f'{key}: {value}')\n",
        "    \n",
        "    return res\n",
        "\n",
        "def read_performance(perf_dir: str):\n",
        "    '''Read inference time from performance benchmark files, and print it'''\n",
        "    csv_sep = ','\n",
        "\n",
        "    # will contain the final mean between each observation's mean\n",
        "    observations_means = []\n",
        "\n",
        "    # do the mean of each observation\n",
        "    for i in range(5000):\n",
        "        perf_filename = f'bench{i}.txt'\n",
        "        perf_file = os.path.join(perf_dir, perf_filename)\n",
        "\n",
        "        if not os.path.isfile(perf_file):\n",
        "            break\n",
        "\n",
        "        # read txt lines (as csv)\n",
        "        file2c = open(perf_file)\n",
        "        doubles = file2c.read().splitlines()\n",
        "        file2c.close()\n",
        "\n",
        "        intra_obs_timelist = []\n",
        "        for entry in doubles: # ori_path, execution_time(s)\n",
        "            ori_path = entry.split(csv_sep)[0]\n",
        "            execution_time = entry.split(csv_sep)[1]\n",
        "            intra_obs_timelist.append(float(execution_time))\n",
        "        \n",
        "        obs_mean = mean(intra_obs_timelist)\n",
        "        obs_mean = '{:.6f}'.format(obs_mean) # round and zerofill\n",
        "        obs_std = pstdev(intra_obs_timelist)\n",
        "        obs_std = '{:.3f}'.format(obs_std) # round and zerofill\n",
        "\n",
        "        obs_string = f'{obs_mean} ± {obs_std}'\n",
        "\n",
        "        observations_means.append(obs_string)\n",
        "        print(f'{perf_dir} at {i}: {obs_string}')\n",
        "    \n",
        "    # get the means from observation means, without the std\n",
        "    obs_mean_values = []\n",
        "    for entry in observations_means:\n",
        "        obs_mean_values.append(float(entry.split(' ')[0]))\n",
        "    \n",
        "    # do the final mean of the observation means\n",
        "    fin_mean = mean(obs_mean_values)\n",
        "    fin_mean = '{:.6f}'.format(fin_mean) # round and zerofill\n",
        "    fin_std = pstdev(obs_mean_values)\n",
        "    fin_std = '{:.3f}'.format(fin_std) # round and zerofill\n",
        "\n",
        "    fin_string = f'{fin_mean} ± {fin_std}'\n",
        "\n",
        "    print(f'{perf_dir} at FIN: {fin_string}\\n')\n",
        "    return fin_string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVuyM4YNVB4h"
      },
      "source": [
        "# Define Latex Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s3RCro6jVB4j"
      },
      "outputs": [],
      "source": [
        "def print_latex(cross_preds: bool, skintones: bool, db_paths: list):\n",
        "    '''\n",
        "    Function used to print latex table featured in thesis\n",
        "\n",
        "    Requires predictions to follow a rigid folder tree structure\n",
        "    '''\n",
        "    if not skintones:\n",
        "        db_list = ['ecu', 'hgr', 'schmugge']\n",
        "    else:\n",
        "        db_list = ['dark', 'medium', 'light']\n",
        "\n",
        "    # paths resolving\n",
        "    if not cross_preds:\n",
        "        detectors = ['skinny', 'probabilistic', 'dyc']\n",
        "        db_paths = []\n",
        "        for db in db_list:\n",
        "            for sd in detectors:\n",
        "                if db == 'hgr' and sd == 'dyc':\n",
        "                    db = 'hgr_small'\n",
        "                if skintones:\n",
        "                    sd += '_st'\n",
        "                db_paths.append(f'dataset/{sd}/base/{db}')\n",
        "    else:\n",
        "        detectors = ['skinny', 'bayes']\n",
        "        db_paths = []\n",
        "        for db_tr in db_list:\n",
        "            for db_te in db_list:\n",
        "                if db_te != db_tr:\n",
        "                    for sd in detectors:\n",
        "                        if skintones:\n",
        "                            sd += '_st'\n",
        "                        db_paths.append(f'dataset/{sd}/cross/{db_tr}_on_{db_te}')\n",
        "\n",
        "    metrics = [f1, iou, dprs]\n",
        "    json_table = []\n",
        "\n",
        "    # compute metrics\n",
        "    for ds in db_paths:\n",
        "        y_path = os.path.join(ds, 'y') # '{dataset}/y'\n",
        "        p_path = os.path.join(ds, 'p') # '{dataset}/p'\n",
        "        \n",
        "        singles = calc_metrics(y_path, p_path, metrics)\n",
        "        # 'dataset/skinny/...'\n",
        "        skin_detector = ds.split('/')[1]\n",
        "\n",
        "        if not cross_preds:\n",
        "            ds = ds + '_on_' + os.path.basename(ds)\n",
        "\n",
        "        table_item = calc_mean_metrics(singles, metrics, desc=ds, method=skin_detector)\n",
        "\n",
        "        json_table.append(table_item)\n",
        "\n",
        "    if not cross_preds:\n",
        "        print(get_latex_base(json_table, db_list))\n",
        "    else:\n",
        "        print(get_latex_cross(json_table, db_list))\n",
        "\n",
        "\n",
        "def is_better(value1, value2, mode: str):\n",
        "    '''Return whether a value is better than another value'''\n",
        "    if mode == 'upper':\n",
        "        return value1 > value2\n",
        "    else:\n",
        "        return value1 < value2\n",
        "\n",
        "def bold_best(data: list, datas: list, base = False):\n",
        "    '''Make the best values bold'''\n",
        "    maxv = {}\n",
        "    # Save the best values between the METHODS (skinny/probabilistic)\n",
        "    # for each metric and dataset combination\n",
        "    for obj in data:\n",
        "        o_m = obj['method']\n",
        "        o_train = obj['train']\n",
        "        o_test = obj['test']\n",
        "        o_f1 = obj['f1']\n",
        "        o_iou = obj['iou']\n",
        "        o_dprs = obj['dprs']\n",
        "        f1iou = float(o_f1.split(' ')[0]) - float(o_iou.split(' ')[0])\n",
        "        obj['f1iou'] = round(f1iou, 4)\n",
        "\n",
        "        print(obj)\n",
        "\n",
        "        # They are cross predictions hence test != train\n",
        "        if base == False and o_train == o_test:\n",
        "            continue\n",
        "        \n",
        "        # For each table metric multirow group\n",
        "        for f_name in ['f1', 'iou', 'dprs', 'f1iou']:\n",
        "            fn_val = obj[f_name] # metric value of the current iteration\n",
        "            fn_idformat = f'{f_name}_{o_train}_{o_test}' # ID format\n",
        "            fnv = f'{fn_idformat}_v' # value of the max measurement\n",
        "            fni = f'{fn_idformat}_i' # ID of the max measurement\n",
        "\n",
        "            bmode = 'upper'\n",
        "            if f_name == 'dprs' or f_name == 'f1iou':\n",
        "                bmode = 'lower'\n",
        "\n",
        "            # For each table column\n",
        "            for trdata in datas:\n",
        "                for tedata in datas:\n",
        "                    if o_train == trdata and o_test == tedata:\n",
        "                        # Save best between methods\n",
        "\n",
        "                        # if max does not exist, add its value and ID\n",
        "                        if fnv not in maxv:\n",
        "                            maxv[fnv] =  fn_val\n",
        "                            maxv[fni] =  o_m\n",
        "                        # if new max, save the measurement and its ID\n",
        "                        elif f_name != 'f1iou' and is_better(float(fn_val.split(' ')[0]), float(maxv[fnv].split(' ')[0]), bmode):\n",
        "                            maxv[fnv] =  fn_val\n",
        "                            maxv[fni] =  o_m\n",
        "                        elif f_name == 'f1iou':\n",
        "                            if is_better(float(fn_val), float(maxv[fnv]), bmode):\n",
        "                                maxv[fnv] =  fn_val\n",
        "                                maxv[fni] =  o_m\n",
        "    newdata = []\n",
        "    # And now make them bold\n",
        "    for obj in data:\n",
        "        o_m = obj['method']\n",
        "        o_train = obj['train']\n",
        "        o_test = obj['test']\n",
        "        o_f1 = obj['f1']\n",
        "        o_iou = obj['iou']\n",
        "        o_dprs = obj['dprs']\n",
        "        f1iou = float(o_f1.split(' ')[0]) - float(o_iou.split(' ')[0])\n",
        "        obj['f1iou'] = '{:.4f}'.format(f1iou) # round and zerofill\n",
        "\n",
        "        for f_name in ['f1', 'iou', 'dprs', 'f1iou']:\n",
        "            fn_val = obj[f_name] # metric value of the current iteration\n",
        "            fn_idformat = f'{f_name}_{o_train}_{o_test}' # ID format\n",
        "            fnv = f'{fn_idformat}_v' # value of the max measurement\n",
        "            fni = f'{fn_idformat}_i' # ID of the max measurement\n",
        "\n",
        "            if fni in maxv and maxv[fni] == o_m:\n",
        "                obj_formatted = '\\\\texttt{' + '\\\\textbf{' + str(obj[f_name]) + '}' + '}'\n",
        "                obj[f_name] = obj_formatted # set bold and monospace\n",
        "                newdata.append(obj)\n",
        "            else:\n",
        "                obj_formatted = '\\\\texttt{' + str(obj[f_name]) + '}'\n",
        "                obj[f_name] = obj_formatted # set monospace\n",
        "                newdata.append(obj)\n",
        "    data = newdata\n",
        "    print('newdata:')\n",
        "    print(data)\n",
        "\n",
        "    # Change JSON format into a standalone data structure containing all table variables\n",
        "    ff = {}\n",
        "    for obj in data:\n",
        "        o_m = obj['method']\n",
        "        o_train = obj['train']\n",
        "        o_test = obj['test']\n",
        "        o_f1 = obj['f1']\n",
        "        o_iou = obj['iou']\n",
        "        o_dprs = obj['dprs']\n",
        "        f1iou = obj['f1iou']\n",
        "\n",
        "        ff[f'f1_{o_m}_{o_train}_{o_test}'] = o_f1\n",
        "        ff[f'iou_{o_m}_{o_train}_{o_test}'] = o_iou\n",
        "        ff[f'dprs_{o_m}_{o_train}_{o_test}'] = o_dprs\n",
        "        ff[f'f1iou_{o_m}_{o_train}_{o_test}'] = f1iou\n",
        "    \n",
        "    print(ff)\n",
        "    return ff\n",
        "\n",
        "# Data is a list of JSON items\n",
        "# JSON item example: {\"name\":\"ecu\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"}\n",
        "def get_latex_cross(data: list, datas = None):\n",
        "    '''Return latex table containing cross-datasets metrics measurements'''\n",
        "    tex_body = ''\n",
        "\n",
        "    if datas == None:\n",
        "        datas = ['ecu', 'hgr', 'schmugge']\n",
        "\n",
        "    ff = bold_best(data, datas)\n",
        "\n",
        "    # Start building the body string\n",
        "    tex_ms = ['F1 $\\\\uparrow$', 'IOU $\\\\uparrow$', 'Dprs $\\\\downarrow$', 'F1 - IOU $\\\\downarrow$']\n",
        "    i = 2\n",
        "    for tm in tex_ms:\n",
        "        mns = tm.split(' ')\n",
        "\n",
        "        if len(mns) > 2: # f1 - iou\n",
        "            mn = 'f1iou'\n",
        "        else:\n",
        "            mn = mns[0].lower()\n",
        "        \n",
        "        # For each metrics there are 2 lines(methods): Skinny and Probabilistic\n",
        "        for j in range(2):\n",
        "            pfix = ''\n",
        "\n",
        "            if j == 0:\n",
        "                met = 'skinny'\n",
        "                mf = met[0].lower()\n",
        "                metf = met.upper() + '\\\\rule{0pt}{14pt}' # spacing between multirows (metrics)\n",
        "                metric_w_arrow = tm\n",
        "                pfix = f'''\\\\multirow{{2}}{{*}}{{{{{metric_w_arrow}}}}}'''\n",
        "            elif j == 1:\n",
        "                met = 'bayes'\n",
        "                mf = met[0].lower()\n",
        "                metf = met.upper()\n",
        "            \n",
        "            if datas != ['ecu', 'hgr', 'schmugge']:\n",
        "                met = f'{met}_st'\n",
        "\n",
        "            # Another data struct to gather all items necessary for writing a table line\n",
        "            tmp = {}\n",
        "            datas_startletter = []\n",
        "            for ds_tr in datas:\n",
        "                datas_startletter.append(ds_tr[0].lower())\n",
        "                for ds_te in datas:\n",
        "                    if ds_tr == ds_te:\n",
        "                        continue\n",
        "                    tmp[f'{mf}_{ds_tr[0].lower()}{ds_te[0].lower()}'] = ff[f'{mn}_{met}_{ds_tr}_{ds_te}']\n",
        "\n",
        "            tex_body += f'{pfix}& {metf}'\n",
        "            for letter_tr in datas_startletter:\n",
        "                for letter_te in datas_startletter:\n",
        "                    if letter_tr != letter_te:\n",
        "                        table_item = tmp[f'{mf}_{letter_tr}{letter_te}']\n",
        "                        tex_body += f' & {table_item}'\n",
        "            tex_body += '\\\\\\\\'\n",
        "    \n",
        "    # String header\n",
        "    tex_header = r'''\n",
        "    \\begin{tabular}{clcccccc}\n",
        "    \\toprule\n",
        "    \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Training}} \n",
        "    '''\n",
        "    \n",
        "    # Add first row\n",
        "    for dss in datas:\n",
        "        tex_header += r'& \\multicolumn{2}{c}{\\head{' + dss.upper() + '}} '\n",
        "    tex_header += r'\\\\'\n",
        "\n",
        "    tex_header += r'\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Testing}} '\n",
        "    # Add second row\n",
        "    for dss in datas:\n",
        "        for dssd in datas:\n",
        "            if dssd != dss:\n",
        "                tex_header += r' & \\multicolumn{1}{c}{\\head{' + dssd.upper() + '}} '\n",
        "    tex_header += r'\\\\'\n",
        "    tex_header += r'\\midrule'\n",
        "\n",
        "    # String end\n",
        "    tex_end = r'''\n",
        "    \\bottomrule\n",
        "    \\end{tabular}\n",
        "    '''\n",
        "\n",
        "    tex = tex_header + tex_body + tex_end\n",
        "    return tex\n",
        "\n",
        "# data is a list of JSON items\n",
        "# JSON item example: {\"name\":\"ecu\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"}\n",
        "def get_latex_base(data: list, datas = None):\n",
        "    '''Return latex table containing base-datasets metrics measurements'''\n",
        "    tex_body = ''\n",
        "\n",
        "    if datas == None:\n",
        "        datas = ['ecu', 'hgr', 'schmugge']\n",
        "\n",
        "    ff = bold_best(data, datas, True)\n",
        "\n",
        "    # Start building the body string\n",
        "    metrics = ['f1', 'iou', 'dprs']\n",
        "    \n",
        "    # At first loop ROWS\n",
        "    # for each metrics there are 2 lines(methods): Skinny and Bayes, DYC\n",
        "    for j in range(3):\n",
        "        if j == 0:\n",
        "            met = 'skinny'\n",
        "        elif j == 1:\n",
        "            met = 'bayes'\n",
        "        else:\n",
        "            met = 'dyc'\n",
        "        metf = met.upper()\n",
        "        mf = met[0].lower()\n",
        "\n",
        "        if datas != ['ecu', 'hgr', 'schmugge']:\n",
        "            met = f'{met}_st'\n",
        "\n",
        "        # Another data struct to gather all items necessary for writing a table line\n",
        "        tmp = {}\n",
        "        # Then loop COLUMNS\n",
        "        datas_startletter = []\n",
        "        for ds in datas:\n",
        "            datas_startletter.append(ds[0].lower())\n",
        "            for tm in metrics:\n",
        "                tmp[f'{mf}_{ds[0].lower()}{tm}'] = ff[f'{tm}_{met}_{ds}_{ds}']\n",
        "\n",
        "        # m is method\n",
        "        # eh = ecu_on_hgr, es = ecu_on_schmugge, ...\n",
        "        tex_body += f'{metf}'\n",
        "        for letter in datas_startletter:\n",
        "            for tmm in metrics:\n",
        "                table_item = tmp[f'{mf}_{letter}{tmm}']\n",
        "                tex_body += f' & {table_item}'\n",
        "        tex_body += '\\\\\\\\'\n",
        "    \n",
        "    # String header\n",
        "    tex_header = r'''\n",
        "    \\begin{tabular}{lccccccccc}\n",
        "    \\toprule\n",
        "    '''\n",
        "    \n",
        "    # Add first row\n",
        "    for dss in datas:\n",
        "        tex_header += r'& \\multicolumn{3}{c}{\\head{' + dss.upper() + '}} '\n",
        "    tex_header += r'\\\\'\n",
        "    \n",
        "    tex_header += r'''\n",
        "    & \\multicolumn{1}{c}{\\head{F1 $\\uparrow$}} & \\multicolumn{1}{c}{\\head{IOU $\\uparrow$}} & \\multicolumn{1}{c}{\\head{Dprs $\\downarrow$}}\n",
        "    & \\multicolumn{1}{c}{\\head{F1 $\\uparrow$}} & \\multicolumn{1}{c}{\\head{IOU $\\uparrow$}} & \\multicolumn{1}{c}{\\head{Dprs $\\downarrow$}}\n",
        "    & \\multicolumn{1}{c}{\\head{F1 $\\uparrow$}} & \\multicolumn{1}{c}{\\head{IOU $\\uparrow$}} & \\multicolumn{1}{c}{\\head{Dprs $\\downarrow$}}\\\\\n",
        "    \\midrule\n",
        "    '''\n",
        "\n",
        "    # String end\n",
        "    tex_end = r'''\n",
        "    \\bottomrule\n",
        "    \\end{tabular}\n",
        "    '''\n",
        "\n",
        "    tex = tex_header + tex_body + tex_end\n",
        "    return tex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSsMpu5gVB4k"
      },
      "source": [
        "# Run Measurements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDs8NXDGdKGV",
        "outputId": "83d2fc57-61ae-4da8-e499-bf98eb687780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "performance/bench_skinny at 0: 0.807026 ± 0.064\n",
            "performance/bench_skinny at 1: 0.797650 ± 0.008\n",
            "performance/bench_skinny at 2: 0.811103 ± 0.010\n",
            "performance/bench_skinny at 3: 0.911956 ± 0.242\n",
            "performance/bench_skinny at 4: 0.805169 ± 0.008\n",
            "performance/bench_skinny at FIN: 0.826581 ± 0.043\n",
            "\n",
            "performance/bench_bayes at 0: 0.459174 ± 0.001\n",
            "performance/bench_bayes at 1: 0.457149 ± 0.003\n",
            "performance/bench_bayes at 2: 0.458998 ± 0.002\n",
            "performance/bench_bayes at 3: 0.458094 ± 0.001\n",
            "performance/bench_bayes at 4: 0.454253 ± 0.002\n",
            "performance/bench_bayes at FIN: 0.457534 ± 0.002\n",
            "\n",
            "performance/bench_dyc at 0: 0.007665 ± 0.000\n",
            "performance/bench_dyc at 1: 0.007677 ± 0.000\n",
            "performance/bench_dyc at 2: 0.007730 ± 0.000\n",
            "performance/bench_dyc at 3: 0.007763 ± 0.000\n",
            "performance/bench_dyc at 4: 0.007752 ± 0.000\n",
            "performance/bench_dyc at FIN: 0.007717 ± 0.000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Print skin detectors' inference times\n",
        "if bench_mode == 'performance':\n",
        "    detectors = ['skinny', 'bayes', 'dyc']\n",
        "\n",
        "    for det in detectors:\n",
        "        perf_dir = f'performance/bench_{det}'\n",
        "        read_performance(perf_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IebRxrN4ZpQX",
        "outputId": "0a032199-73ef-46b1-aa48-7eb1f438f6a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1558/1558 [00:11<00:00, 138.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 1558 matches\n",
            "dataset/skinny/cross/ecu_on_hgr\n",
            "dprs: 0.1098 ± 0.15\n",
            "f1: 0.9308 ± 0.11\n",
            "iou: 0.8851 ± 0.15\n",
            "method: skinny\n",
            "test: hgr\n",
            "train: ecu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1558/1558 [00:11<00:00, 134.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 1558 matches\n",
            "dataset/bayes/cross/ecu_on_hgr\n",
            "dprs: 0.5701 ± 0.29\n",
            "f1: 0.5577 ± 0.29\n",
            "iou: 0.4393 ± 0.27\n",
            "method: bayes\n",
            "test: hgr\n",
            "train: ecu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 840/840 [00:01<00:00, 444.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 840 matches\n",
            "dataset/skinny/cross/ecu_on_schmugge\n",
            "dprs: 0.7570 ± 0.56\n",
            "f1: 0.4625 ± 0.41\n",
            "iou: 0.3986 ± 0.37\n",
            "method: skinny\n",
            "test: schmugge\n",
            "train: ecu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 840/840 [00:01<00:00, 553.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 840 matches\n",
            "dataset/bayes/cross/ecu_on_schmugge\n",
            "dprs: 1.0477 ± 0.35\n",
            "f1: 0.3319 ± 0.28\n",
            "iou: 0.2346 ± 0.21\n",
            "method: bayes\n",
            "test: schmugge\n",
            "train: ecu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3998/3998 [00:32<00:00, 123.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 3998 matches\n",
            "dataset/skinny/cross/hgr_on_ecu\n",
            "dprs: 0.3913 ± 0.26\n",
            "f1: 0.7252 ± 0.20\n",
            "iou: 0.6038 ± 0.22\n",
            "method: skinny\n",
            "test: ecu\n",
            "train: hgr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3998/3998 [00:56<00:00, 70.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 3998 matches\n",
            "dataset/bayes/cross/hgr_on_ecu\n",
            "dprs: 0.8830 ± 0.23\n",
            "f1: 0.4279 ± 0.19\n",
            "iou: 0.2929 ± 0.17\n",
            "method: bayes\n",
            "test: ecu\n",
            "train: hgr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 840/840 [00:02<00:00, 408.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 840 matches\n",
            "dataset/skinny/cross/hgr_on_schmugge\n",
            "dprs: 0.9695 ± 0.44\n",
            "f1: 0.2918 ± 0.31\n",
            "iou: 0.2168 ± 0.25\n",
            "method: skinny\n",
            "test: schmugge\n",
            "train: hgr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 840/840 [00:01<00:00, 520.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 840 matches\n",
            "dataset/bayes/cross/hgr_on_schmugge\n",
            "dprs: 1.0219 ± 0.42\n",
            "f1: 0.4000 ± 0.32\n",
            "iou: 0.2981 ± 0.24\n",
            "method: bayes\n",
            "test: schmugge\n",
            "train: hgr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3998/3998 [00:35<00:00, 111.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 3998 matches\n",
            "dataset/skinny/cross/schmugge_on_ecu\n",
            "dprs: 0.5537 ± 0.27\n",
            "f1: 0.6133 ± 0.21\n",
            "iou: 0.4754 ± 0.22\n",
            "method: skinny\n",
            "test: ecu\n",
            "train: schmugge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3998/3998 [00:52<00:00, 75.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 3998 matches\n",
            "dataset/bayes/cross/schmugge_on_ecu\n",
            "dprs: 0.7542 ± 0.30\n",
            "f1: 0.4638 ± 0.23\n",
            "iou: 0.3318 ± 0.20\n",
            "method: bayes\n",
            "test: ecu\n",
            "train: schmugge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1558/1558 [00:12<00:00, 124.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 1558 matches\n",
            "dataset/skinny/cross/schmugge_on_hgr\n",
            "dprs: 0.2846 ± 0.27\n",
            "f1: 0.8106 ± 0.19\n",
            "iou: 0.7191 ± 0.23\n",
            "method: skinny\n",
            "test: hgr\n",
            "train: schmugge\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1558/1558 [00:12<00:00, 124.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Found 1558 matches\n",
            "dataset/bayes/cross/schmugge_on_hgr\n",
            "dprs: 0.6523 ± 0.27\n",
            "f1: 0.5060 ± 0.25\n",
            "iou: 0.3752 ± 0.22\n",
            "method: bayes\n",
            "test: hgr\n",
            "train: schmugge\n",
            "{'method': 'skinny', 'train': 'ecu', 'test': 'hgr', 'f1': '0.9308 ± 0.11', 'iou': '0.8851 ± 0.15', 'dprs': '0.1098 ± 0.15', 'f1iou': 0.0457}\n",
            "{'method': 'bayes', 'train': 'ecu', 'test': 'hgr', 'f1': '0.5577 ± 0.29', 'iou': '0.4393 ± 0.27', 'dprs': '0.5701 ± 0.29', 'f1iou': 0.1184}\n",
            "{'method': 'skinny', 'train': 'ecu', 'test': 'schmugge', 'f1': '0.4625 ± 0.41', 'iou': '0.3986 ± 0.37', 'dprs': '0.7570 ± 0.56', 'f1iou': 0.0639}\n",
            "{'method': 'bayes', 'train': 'ecu', 'test': 'schmugge', 'f1': '0.3319 ± 0.28', 'iou': '0.2346 ± 0.21', 'dprs': '1.0477 ± 0.35', 'f1iou': 0.0973}\n",
            "{'method': 'skinny', 'train': 'hgr', 'test': 'ecu', 'f1': '0.7252 ± 0.20', 'iou': '0.6038 ± 0.22', 'dprs': '0.3913 ± 0.26', 'f1iou': 0.1214}\n",
            "{'method': 'bayes', 'train': 'hgr', 'test': 'ecu', 'f1': '0.4279 ± 0.19', 'iou': '0.2929 ± 0.17', 'dprs': '0.8830 ± 0.23', 'f1iou': 0.135}\n",
            "{'method': 'skinny', 'train': 'hgr', 'test': 'schmugge', 'f1': '0.2918 ± 0.31', 'iou': '0.2168 ± 0.25', 'dprs': '0.9695 ± 0.44', 'f1iou': 0.075}\n",
            "{'method': 'bayes', 'train': 'hgr', 'test': 'schmugge', 'f1': '0.4000 ± 0.32', 'iou': '0.2981 ± 0.24', 'dprs': '1.0219 ± 0.42', 'f1iou': 0.1019}\n",
            "{'method': 'skinny', 'train': 'schmugge', 'test': 'ecu', 'f1': '0.6133 ± 0.21', 'iou': '0.4754 ± 0.22', 'dprs': '0.5537 ± 0.27', 'f1iou': 0.1379}\n",
            "{'method': 'bayes', 'train': 'schmugge', 'test': 'ecu', 'f1': '0.4638 ± 0.23', 'iou': '0.3318 ± 0.20', 'dprs': '0.7542 ± 0.30', 'f1iou': 0.132}\n",
            "{'method': 'skinny', 'train': 'schmugge', 'test': 'hgr', 'f1': '0.8106 ± 0.19', 'iou': '0.7191 ± 0.23', 'dprs': '0.2846 ± 0.27', 'f1iou': 0.0915}\n",
            "{'method': 'bayes', 'train': 'schmugge', 'test': 'hgr', 'f1': '0.5060 ± 0.25', 'iou': '0.3752 ± 0.22', 'dprs': '0.6523 ± 0.27', 'f1iou': 0.1308}\n",
            "newdata:\n",
            "[{'method': 'skinny', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.9308 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8851 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.1098 ± 0.15}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0457}}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.9308 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8851 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.1098 ± 0.15}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0457}}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.9308 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8851 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.1098 ± 0.15}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0457}}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.9308 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8851 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.1098 ± 0.15}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0457}}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{0.5577 ± 0.29}', 'iou': '\\\\texttt{0.4393 ± 0.27}', 'dprs': '\\\\texttt{0.5701 ± 0.29}', 'f1iou': '\\\\texttt{0.1184}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{0.5577 ± 0.29}', 'iou': '\\\\texttt{0.4393 ± 0.27}', 'dprs': '\\\\texttt{0.5701 ± 0.29}', 'f1iou': '\\\\texttt{0.1184}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{0.5577 ± 0.29}', 'iou': '\\\\texttt{0.4393 ± 0.27}', 'dprs': '\\\\texttt{0.5701 ± 0.29}', 'f1iou': '\\\\texttt{0.1184}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'hgr', 'f1': '\\\\texttt{0.5577 ± 0.29}', 'iou': '\\\\texttt{0.4393 ± 0.27}', 'dprs': '\\\\texttt{0.5701 ± 0.29}', 'f1iou': '\\\\texttt{0.1184}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4625 ± 0.41}}', 'iou': '\\\\texttt{\\\\textbf{0.3986 ± 0.37}}', 'dprs': '\\\\texttt{\\\\textbf{0.7570 ± 0.56}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0639}}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4625 ± 0.41}}', 'iou': '\\\\texttt{\\\\textbf{0.3986 ± 0.37}}', 'dprs': '\\\\texttt{\\\\textbf{0.7570 ± 0.56}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0639}}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4625 ± 0.41}}', 'iou': '\\\\texttt{\\\\textbf{0.3986 ± 0.37}}', 'dprs': '\\\\texttt{\\\\textbf{0.7570 ± 0.56}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0639}}'}, {'method': 'skinny', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4625 ± 0.41}}', 'iou': '\\\\texttt{\\\\textbf{0.3986 ± 0.37}}', 'dprs': '\\\\texttt{\\\\textbf{0.7570 ± 0.56}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0639}}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{0.3319 ± 0.28}', 'iou': '\\\\texttt{0.2346 ± 0.21}', 'dprs': '\\\\texttt{1.0477 ± 0.35}', 'f1iou': '\\\\texttt{0.0973}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{0.3319 ± 0.28}', 'iou': '\\\\texttt{0.2346 ± 0.21}', 'dprs': '\\\\texttt{1.0477 ± 0.35}', 'f1iou': '\\\\texttt{0.0973}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{0.3319 ± 0.28}', 'iou': '\\\\texttt{0.2346 ± 0.21}', 'dprs': '\\\\texttt{1.0477 ± 0.35}', 'f1iou': '\\\\texttt{0.0973}'}, {'method': 'bayes', 'train': 'ecu', 'test': 'schmugge', 'f1': '\\\\texttt{0.3319 ± 0.28}', 'iou': '\\\\texttt{0.2346 ± 0.21}', 'dprs': '\\\\texttt{1.0477 ± 0.35}', 'f1iou': '\\\\texttt{0.0973}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.7252 ± 0.20}}', 'iou': '\\\\texttt{\\\\textbf{0.6038 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.3913 ± 0.26}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1214}}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.7252 ± 0.20}}', 'iou': '\\\\texttt{\\\\textbf{0.6038 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.3913 ± 0.26}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1214}}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.7252 ± 0.20}}', 'iou': '\\\\texttt{\\\\textbf{0.6038 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.3913 ± 0.26}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1214}}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.7252 ± 0.20}}', 'iou': '\\\\texttt{\\\\textbf{0.6038 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.3913 ± 0.26}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1214}}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{0.4279 ± 0.19}', 'iou': '\\\\texttt{0.2929 ± 0.17}', 'dprs': '\\\\texttt{0.8830 ± 0.23}', 'f1iou': '\\\\texttt{0.1350}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{0.4279 ± 0.19}', 'iou': '\\\\texttt{0.2929 ± 0.17}', 'dprs': '\\\\texttt{0.8830 ± 0.23}', 'f1iou': '\\\\texttt{0.1350}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{0.4279 ± 0.19}', 'iou': '\\\\texttt{0.2929 ± 0.17}', 'dprs': '\\\\texttt{0.8830 ± 0.23}', 'f1iou': '\\\\texttt{0.1350}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'ecu', 'f1': '\\\\texttt{0.4279 ± 0.19}', 'iou': '\\\\texttt{0.2929 ± 0.17}', 'dprs': '\\\\texttt{0.8830 ± 0.23}', 'f1iou': '\\\\texttt{0.1350}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{0.2918 ± 0.31}', 'iou': '\\\\texttt{0.2168 ± 0.25}', 'dprs': '\\\\texttt{\\\\textbf{0.9695 ± 0.44}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0750}}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{0.2918 ± 0.31}', 'iou': '\\\\texttt{0.2168 ± 0.25}', 'dprs': '\\\\texttt{\\\\textbf{0.9695 ± 0.44}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0750}}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{0.2918 ± 0.31}', 'iou': '\\\\texttt{0.2168 ± 0.25}', 'dprs': '\\\\texttt{\\\\textbf{0.9695 ± 0.44}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0750}}'}, {'method': 'skinny', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{0.2918 ± 0.31}', 'iou': '\\\\texttt{0.2168 ± 0.25}', 'dprs': '\\\\texttt{\\\\textbf{0.9695 ± 0.44}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0750}}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4000 ± 0.32}}', 'iou': '\\\\texttt{\\\\textbf{0.2981 ± 0.24}}', 'dprs': '\\\\texttt{1.0219 ± 0.42}', 'f1iou': '\\\\texttt{0.1019}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4000 ± 0.32}}', 'iou': '\\\\texttt{\\\\textbf{0.2981 ± 0.24}}', 'dprs': '\\\\texttt{1.0219 ± 0.42}', 'f1iou': '\\\\texttt{0.1019}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4000 ± 0.32}}', 'iou': '\\\\texttt{\\\\textbf{0.2981 ± 0.24}}', 'dprs': '\\\\texttt{1.0219 ± 0.42}', 'f1iou': '\\\\texttt{0.1019}'}, {'method': 'bayes', 'train': 'hgr', 'test': 'schmugge', 'f1': '\\\\texttt{\\\\textbf{0.4000 ± 0.32}}', 'iou': '\\\\texttt{\\\\textbf{0.2981 ± 0.24}}', 'dprs': '\\\\texttt{1.0219 ± 0.42}', 'f1iou': '\\\\texttt{0.1019}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.6133 ± 0.21}}', 'iou': '\\\\texttt{\\\\textbf{0.4754 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.5537 ± 0.27}}', 'f1iou': '\\\\texttt{0.1379}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.6133 ± 0.21}}', 'iou': '\\\\texttt{\\\\textbf{0.4754 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.5537 ± 0.27}}', 'f1iou': '\\\\texttt{0.1379}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.6133 ± 0.21}}', 'iou': '\\\\texttt{\\\\textbf{0.4754 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.5537 ± 0.27}}', 'f1iou': '\\\\texttt{0.1379}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{\\\\textbf{0.6133 ± 0.21}}', 'iou': '\\\\texttt{\\\\textbf{0.4754 ± 0.22}}', 'dprs': '\\\\texttt{\\\\textbf{0.5537 ± 0.27}}', 'f1iou': '\\\\texttt{0.1379}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{0.4638 ± 0.23}', 'iou': '\\\\texttt{0.3318 ± 0.20}', 'dprs': '\\\\texttt{0.7542 ± 0.30}', 'f1iou': '\\\\texttt{\\\\textbf{0.1320}}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{0.4638 ± 0.23}', 'iou': '\\\\texttt{0.3318 ± 0.20}', 'dprs': '\\\\texttt{0.7542 ± 0.30}', 'f1iou': '\\\\texttt{\\\\textbf{0.1320}}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{0.4638 ± 0.23}', 'iou': '\\\\texttt{0.3318 ± 0.20}', 'dprs': '\\\\texttt{0.7542 ± 0.30}', 'f1iou': '\\\\texttt{\\\\textbf{0.1320}}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'ecu', 'f1': '\\\\texttt{0.4638 ± 0.23}', 'iou': '\\\\texttt{0.3318 ± 0.20}', 'dprs': '\\\\texttt{0.7542 ± 0.30}', 'f1iou': '\\\\texttt{\\\\textbf{0.1320}}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.8106 ± 0.19}}', 'iou': '\\\\texttt{\\\\textbf{0.7191 ± 0.23}}', 'dprs': '\\\\texttt{\\\\textbf{0.2846 ± 0.27}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0915}}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.8106 ± 0.19}}', 'iou': '\\\\texttt{\\\\textbf{0.7191 ± 0.23}}', 'dprs': '\\\\texttt{\\\\textbf{0.2846 ± 0.27}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0915}}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.8106 ± 0.19}}', 'iou': '\\\\texttt{\\\\textbf{0.7191 ± 0.23}}', 'dprs': '\\\\texttt{\\\\textbf{0.2846 ± 0.27}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0915}}'}, {'method': 'skinny', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{\\\\textbf{0.8106 ± 0.19}}', 'iou': '\\\\texttt{\\\\textbf{0.7191 ± 0.23}}', 'dprs': '\\\\texttt{\\\\textbf{0.2846 ± 0.27}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0915}}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{0.5060 ± 0.25}', 'iou': '\\\\texttt{0.3752 ± 0.22}', 'dprs': '\\\\texttt{0.6523 ± 0.27}', 'f1iou': '\\\\texttt{0.1308}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{0.5060 ± 0.25}', 'iou': '\\\\texttt{0.3752 ± 0.22}', 'dprs': '\\\\texttt{0.6523 ± 0.27}', 'f1iou': '\\\\texttt{0.1308}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{0.5060 ± 0.25}', 'iou': '\\\\texttt{0.3752 ± 0.22}', 'dprs': '\\\\texttt{0.6523 ± 0.27}', 'f1iou': '\\\\texttt{0.1308}'}, {'method': 'bayes', 'train': 'schmugge', 'test': 'hgr', 'f1': '\\\\texttt{0.5060 ± 0.25}', 'iou': '\\\\texttt{0.3752 ± 0.22}', 'dprs': '\\\\texttt{0.6523 ± 0.27}', 'f1iou': '\\\\texttt{0.1308}'}]\n",
            "{'f1_skinny_ecu_hgr': '\\\\texttt{\\\\textbf{0.9308 ± 0.11}}', 'iou_skinny_ecu_hgr': '\\\\texttt{\\\\textbf{0.8851 ± 0.15}}', 'dprs_skinny_ecu_hgr': '\\\\texttt{\\\\textbf{0.1098 ± 0.15}}', 'f1iou_skinny_ecu_hgr': '\\\\texttt{\\\\textbf{0.0457}}', 'f1_bayes_ecu_hgr': '\\\\texttt{0.5577 ± 0.29}', 'iou_bayes_ecu_hgr': '\\\\texttt{0.4393 ± 0.27}', 'dprs_bayes_ecu_hgr': '\\\\texttt{0.5701 ± 0.29}', 'f1iou_bayes_ecu_hgr': '\\\\texttt{0.1184}', 'f1_skinny_ecu_schmugge': '\\\\texttt{\\\\textbf{0.4625 ± 0.41}}', 'iou_skinny_ecu_schmugge': '\\\\texttt{\\\\textbf{0.3986 ± 0.37}}', 'dprs_skinny_ecu_schmugge': '\\\\texttt{\\\\textbf{0.7570 ± 0.56}}', 'f1iou_skinny_ecu_schmugge': '\\\\texttt{\\\\textbf{0.0639}}', 'f1_bayes_ecu_schmugge': '\\\\texttt{0.3319 ± 0.28}', 'iou_bayes_ecu_schmugge': '\\\\texttt{0.2346 ± 0.21}', 'dprs_bayes_ecu_schmugge': '\\\\texttt{1.0477 ± 0.35}', 'f1iou_bayes_ecu_schmugge': '\\\\texttt{0.0973}', 'f1_skinny_hgr_ecu': '\\\\texttt{\\\\textbf{0.7252 ± 0.20}}', 'iou_skinny_hgr_ecu': '\\\\texttt{\\\\textbf{0.6038 ± 0.22}}', 'dprs_skinny_hgr_ecu': '\\\\texttt{\\\\textbf{0.3913 ± 0.26}}', 'f1iou_skinny_hgr_ecu': '\\\\texttt{\\\\textbf{0.1214}}', 'f1_bayes_hgr_ecu': '\\\\texttt{0.4279 ± 0.19}', 'iou_bayes_hgr_ecu': '\\\\texttt{0.2929 ± 0.17}', 'dprs_bayes_hgr_ecu': '\\\\texttt{0.8830 ± 0.23}', 'f1iou_bayes_hgr_ecu': '\\\\texttt{0.1350}', 'f1_skinny_hgr_schmugge': '\\\\texttt{0.2918 ± 0.31}', 'iou_skinny_hgr_schmugge': '\\\\texttt{0.2168 ± 0.25}', 'dprs_skinny_hgr_schmugge': '\\\\texttt{\\\\textbf{0.9695 ± 0.44}}', 'f1iou_skinny_hgr_schmugge': '\\\\texttt{\\\\textbf{0.0750}}', 'f1_bayes_hgr_schmugge': '\\\\texttt{\\\\textbf{0.4000 ± 0.32}}', 'iou_bayes_hgr_schmugge': '\\\\texttt{\\\\textbf{0.2981 ± 0.24}}', 'dprs_bayes_hgr_schmugge': '\\\\texttt{1.0219 ± 0.42}', 'f1iou_bayes_hgr_schmugge': '\\\\texttt{0.1019}', 'f1_skinny_schmugge_ecu': '\\\\texttt{\\\\textbf{0.6133 ± 0.21}}', 'iou_skinny_schmugge_ecu': '\\\\texttt{\\\\textbf{0.4754 ± 0.22}}', 'dprs_skinny_schmugge_ecu': '\\\\texttt{\\\\textbf{0.5537 ± 0.27}}', 'f1iou_skinny_schmugge_ecu': '\\\\texttt{0.1379}', 'f1_bayes_schmugge_ecu': '\\\\texttt{0.4638 ± 0.23}', 'iou_bayes_schmugge_ecu': '\\\\texttt{0.3318 ± 0.20}', 'dprs_bayes_schmugge_ecu': '\\\\texttt{0.7542 ± 0.30}', 'f1iou_bayes_schmugge_ecu': '\\\\texttt{\\\\textbf{0.1320}}', 'f1_skinny_schmugge_hgr': '\\\\texttt{\\\\textbf{0.8106 ± 0.19}}', 'iou_skinny_schmugge_hgr': '\\\\texttt{\\\\textbf{0.7191 ± 0.23}}', 'dprs_skinny_schmugge_hgr': '\\\\texttt{\\\\textbf{0.2846 ± 0.27}}', 'f1iou_skinny_schmugge_hgr': '\\\\texttt{\\\\textbf{0.0915}}', 'f1_bayes_schmugge_hgr': '\\\\texttt{0.5060 ± 0.25}', 'iou_bayes_schmugge_hgr': '\\\\texttt{0.3752 ± 0.22}', 'dprs_bayes_schmugge_hgr': '\\\\texttt{0.6523 ± 0.27}', 'f1iou_bayes_schmugge_hgr': '\\\\texttt{0.1308}'}\n",
            "\n",
            "    \\begin{tabular}{clcccccc}\n",
            "    \\toprule\n",
            "    \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Training}} \n",
            "    & \\multicolumn{2}{c}{\\head{ECU}} & \\multicolumn{2}{c}{\\head{HGR}} & \\multicolumn{2}{c}{\\head{SCHMUGGE}} \\\\\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Testing}}  & \\multicolumn{1}{c}{\\head{HGR}}  & \\multicolumn{1}{c}{\\head{SCHMUGGE}}  & \\multicolumn{1}{c}{\\head{ECU}}  & \\multicolumn{1}{c}{\\head{SCHMUGGE}}  & \\multicolumn{1}{c}{\\head{ECU}}  & \\multicolumn{1}{c}{\\head{HGR}} \\\\\\midrule\\multirow{2}{*}{{F1 $\\uparrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{\\textbf{0.9308 ± 0.11}} & \\texttt{\\textbf{0.4625 ± 0.41}} & \\texttt{\\textbf{0.7252 ± 0.20}} & \\texttt{0.2918 ± 0.31} & \\texttt{\\textbf{0.6133 ± 0.21}} & \\texttt{\\textbf{0.8106 ± 0.19}}\\\\& BAYES & \\texttt{0.5577 ± 0.29} & \\texttt{0.3319 ± 0.28} & \\texttt{0.4279 ± 0.19} & \\texttt{\\textbf{0.4000 ± 0.32}} & \\texttt{0.4638 ± 0.23} & \\texttt{0.5060 ± 0.25}\\\\\\multirow{2}{*}{{IOU $\\uparrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{\\textbf{0.8851 ± 0.15}} & \\texttt{\\textbf{0.3986 ± 0.37}} & \\texttt{\\textbf{0.6038 ± 0.22}} & \\texttt{0.2168 ± 0.25} & \\texttt{\\textbf{0.4754 ± 0.22}} & \\texttt{\\textbf{0.7191 ± 0.23}}\\\\& BAYES & \\texttt{0.4393 ± 0.27} & \\texttt{0.2346 ± 0.21} & \\texttt{0.2929 ± 0.17} & \\texttt{\\textbf{0.2981 ± 0.24}} & \\texttt{0.3318 ± 0.20} & \\texttt{0.3752 ± 0.22}\\\\\\multirow{2}{*}{{Dprs $\\downarrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{\\textbf{0.1098 ± 0.15}} & \\texttt{\\textbf{0.7570 ± 0.56}} & \\texttt{\\textbf{0.3913 ± 0.26}} & \\texttt{\\textbf{0.9695 ± 0.44}} & \\texttt{\\textbf{0.5537 ± 0.27}} & \\texttt{\\textbf{0.2846 ± 0.27}}\\\\& BAYES & \\texttt{0.5701 ± 0.29} & \\texttt{1.0477 ± 0.35} & \\texttt{0.8830 ± 0.23} & \\texttt{1.0219 ± 0.42} & \\texttt{0.7542 ± 0.30} & \\texttt{0.6523 ± 0.27}\\\\\\multirow{2}{*}{{F1 - IOU $\\downarrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{\\textbf{0.0457}} & \\texttt{\\textbf{0.0639}} & \\texttt{\\textbf{0.1214}} & \\texttt{\\textbf{0.0750}} & \\texttt{0.1379} & \\texttt{\\textbf{0.0915}}\\\\& BAYES & \\texttt{0.1184} & \\texttt{0.0973} & \\texttt{0.1350} & \\texttt{0.1019} & \\texttt{\\textbf{0.1320}} & \\texttt{0.1308}\\\\\n",
            "    \\bottomrule\n",
            "    \\end{tabular}\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "\n",
        "#  Settings\n",
        "\n",
        "#  Method can be: base, cross\n",
        "method = 'cross'\n",
        "#  Mode can be: normal, skintones\n",
        "mode = 'normal'\n",
        "#  Whether to dump results to a json file\n",
        "dump_to_file = False\n",
        "\n",
        "\n",
        "\n",
        "#  Print skin detectors' performance on datasets\n",
        "if bench_mode == 'dataset':\n",
        "    #  Resolve datasets\n",
        "    if mode == 'normal':\n",
        "        db_list = ['ecu', 'hgr', 'schmugge']\n",
        "    elif mode == 'skintones':\n",
        "        db_list = ['dark', 'medium', 'light']\n",
        "\n",
        "\n",
        "    #  Resolve paths\n",
        "    if method == 'base':\n",
        "        detectors = ['skinny', 'bayes', 'dyc']\n",
        "        db_paths = []\n",
        "        for db in db_list:\n",
        "            for sd in detectors:\n",
        "                if db == 'hgr' and sd == 'dyc':\n",
        "                    db = 'hgr_small'\n",
        "                if mode == 'skintones':\n",
        "                    sd += '_st'\n",
        "                db_paths.append(f'dataset/{sd}/base/{db}')\n",
        "    elif method == 'cross':\n",
        "        detectors = ['skinny', 'bayes']\n",
        "        db_skinny = []\n",
        "        db_bayes = []\n",
        "        db_paths = []\n",
        "        for db_tr in db_list:\n",
        "            for db_te in db_list:\n",
        "                if db_te != db_tr:\n",
        "                    for sd in detectors:\n",
        "                        if mode == 'skintones':\n",
        "                            sd += '_st'\n",
        "                        db_paths.append(f'dataset/{sd}/cross/{db_tr}_on_{db_te}')\n",
        "\n",
        "\n",
        "    #  Compute metrics\n",
        "    metrics = [f1, iou, dprs] #  metrics featured in the thesis\n",
        "    json_table = []\n",
        "    for ds in db_paths:\n",
        "        y_path = os.path.join(ds, 'y') # '{dataset}/y'\n",
        "        p_path = os.path.join(ds, 'p') # '{dataset}/p'\n",
        "        \n",
        "        singles = calc_metrics(y_path, p_path, metrics)\n",
        "        # 'dataset/skinny/...'\n",
        "        skin_detector = ds.split('/')[1]\n",
        "\n",
        "        if method == 'base':\n",
        "            ds = ds + '_on_' + os.path.basename(ds)\n",
        "\n",
        "        avg = calc_mean_metrics(singles, metrics, desc=ds, method=skin_detector)\n",
        "        json_table.append(avg)\n",
        "\n",
        "    if method == 'base':\n",
        "        print(get_latex_base(json_table, db_list))\n",
        "    else:\n",
        "        print(get_latex_cross(json_table, db_list))\n",
        "\n",
        "\n",
        "    #  Save JSON table\n",
        "    if dump_to_file == True:\n",
        "        out_table = open(\"metrics.json\", \"w\")\n",
        "        json.dump(json_table, out_table)\n",
        "        out_table.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4osjc7kzVB4n"
      },
      "source": [
        "# Unittest Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q1qydB8VB4n",
        "outputId": "7f3f8a6e-b20b-4516-ca25-110030659330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 0 tests in 0.000s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fc77c0e4110>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import unittest\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TestMetrics(unittest.TestCase):\n",
        "    '''Unit testing for metrics measurements'''\n",
        "\n",
        "\n",
        "    def metrics_unittesting(self):\n",
        "        # singles\n",
        "        y_true = np.array([[1,0,0],[0,1,0],[0,0,1]]) > 0 # > 0 to cast as bool\n",
        "        y_pred = np.array([[1,0,0],[1,0,0],[0,0,0]]) > 0\n",
        "        tp = 1\n",
        "        tn = 5\n",
        "        fp = 1\n",
        "        fn = 2\n",
        "        cs = confmat_scores(y_true, y_pred)\n",
        "        pr = tp / (tp+fp) # 0.5\n",
        "        re = tp / (tp+fn) # 0.33\n",
        "        sp = tn / (tn+fp) # 0.83\n",
        "        f1_ = 2*re*pr / (re+pr)\n",
        "        overlap = y_true * y_pred\n",
        "        union =   y_true | y_pred # with bitwise or it would work even without casting matrix as bool\n",
        "        iou_ = overlap.sum() / (union.sum())\n",
        "        a = (1 - pr)**2 # 0.25\n",
        "        b = (1 - re)**2 # 0.4489\n",
        "        c = (1 - sp)**2 # 0.0289\n",
        "        dprs_ = math.sqrt(a + b + c)\n",
        "        #           3               /  V        2       *    3      *     6     *    7\n",
        "        mcc_ = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "        print(cs)\n",
        "        self.assertEqual(round(pr, 2), 0.5)\n",
        "        self.assertEqual(round(pr, 2), round(precision(cs), 2), 'pr not equal to its function')\n",
        "        self.assertEqual(round(re, 2), 0.33)\n",
        "        self.assertEqual(round(re, 2), round(recall(cs), 2), 're not equal to its function')\n",
        "        self.assertEqual(round(sp, 2), 0.83)\n",
        "        self.assertEqual(round(sp, 2), round(specificity(cs), 2), 'sp not equal to its function')\n",
        "        self.assertEqual(round(f1_, 2), 0.40)\n",
        "        self.assertEqual(round(f1_, 2), round(f1(cs), 2), 'f1 not equal to its function')\n",
        "        self.assertEqual(round(iou_, 2), 0.25)\n",
        "        self.assertEqual(round(iou_, 2), round(iou(cs), 2), 'iou not equal to its function')\n",
        "        self.assertEqual(round(iou_, 2), round(iou_logical(y_true, y_pred), 2), 'iou not equal to its function (alt)')\n",
        "        self.assertEqual(round(dprs_, 2), 0.85)\n",
        "        self.assertEqual(round(dprs_, 2), round(dprs(cs), 2), 'dprs not equal to its function')\n",
        "        self.assertEqual(round(mcc_, 2), 0.19)\n",
        "        self.assertEqual(round(mcc_, 2), round(mcc(cs), 2), 'mcc not equal to its function')\n",
        "        # medium avg\n",
        "        #metrics = [f1_medium, f1, f2, iou, iou_logical, dprs_medium, dprs, mcc, recall, precision, specificity]\n",
        "        #rpd = pd_metrics(docs_y_path, docs_p_path, metrics)\n",
        "        #res = print_pd_mean(rpd, metrics, desc='unit testing')\n",
        "        pr_1 = 0.51\n",
        "        pr_2 = 0.82\n",
        "        pr_avg = (pr_1 + pr_2) /2   # 0.67\n",
        "        re_1 = 0.61\n",
        "        re_2 = 0.45\n",
        "        re_avg = (re_1 + re_2) /2   # 0.53\n",
        "        sp_1 = 0.14\n",
        "        sp_2 = 0.62\n",
        "        sp_avg = (sp_1 + sp_2) /2   # 0.38\n",
        "        f1_med_avg = pr_avg * re_avg * 2 / (pr_avg + re_avg)\n",
        "        a_ = (1 - pr_avg)**2 # 0.10\n",
        "        b_ = (1 - re_avg)**2 # 0.22\n",
        "        c_ = (1 - sp_avg)**2 # 0.38\n",
        "        dprs_med_avg = math.sqrt(a_ + b_ + c_)\n",
        "        self.assertEqual(round(f1_med_avg, 2), 0.59)\n",
        "        self.assertEqual(round(f1_med_avg, 2), round(f1_medium(pr_avg, re_avg, sp_avg), 2), 'f1-medium not equal to its function')\n",
        "        self.assertEqual(round(dprs_med_avg, 2), 0.85)\n",
        "        self.assertEqual(round(dprs_med_avg, 2), round(dprs_medium(pr_avg, re_avg, sp_avg), 2), 'dprs-medium not equal to its function')\n",
        "\n",
        "    def mcc_unittest(self):\n",
        "        '''\n",
        "        Unit testing based on MCC's paper analysis\n",
        "        \n",
        "        The analysis shows how F1 doesn't care much about TN and could signal\n",
        "        over-optimistic data to the classifier\n",
        "        '''\n",
        "        # Use Case A1: Positively imbalanced dataset\n",
        "        data = {}\n",
        "        data['ap'] = 91   # 91 sick patients\n",
        "        data['an'] = 9    # 9 healthy individuals\n",
        "        data['se'] = 99\n",
        "        data['tp'] = 90   # algorithm is good at predicting positive data\n",
        "        data['fp'] = 9\n",
        "        data['tn'] = 0\n",
        "        data['fn'] = 1    # algorithm is bad at predicting negative data\n",
        "        # F1 measures an almost perfect score, MCC instead measures a bad score\n",
        "        # F1 0.95    MCC -0.03\n",
        "        f1_ = round(f1(data), 2)\n",
        "        mcc_ = round(mcc(data), 2)\n",
        "        self.assertEqual(f1_, 0.95)\n",
        "        self.assertEqual(mcc_, -0.03)\n",
        "\n",
        "        # Use Case A2: Positively imbalanced dataset\n",
        "        data = {}\n",
        "        data['ap'] = 75   # 75 positives\n",
        "        data['an'] = 25   # 25 negatives\n",
        "        data['se'] = 11\n",
        "        data['tp'] = 5    # classifier unable to predict positives\n",
        "        data['fp'] = 6\n",
        "        data['tn'] = 19   # classifier was able to predict negatives\n",
        "        data['fn'] = 70\n",
        "        # In this case both the metrics measure a bad score\n",
        "        # F1 0.12    MCC -0.24\n",
        "        f1_ = round(f1(data), 2)\n",
        "        mcc_ = round(mcc(data), 2)\n",
        "        self.assertEqual(f1_, 0.12)\n",
        "        self.assertEqual(mcc_, -0.24)\n",
        "\n",
        "        # Use Case B1: Balanced dataset\n",
        "        data = {}\n",
        "        data['ap'] = 50   # 50 positives\n",
        "        data['an'] = 50   # 50 negatives\n",
        "        data['se'] = 92\n",
        "        data['tp'] = 47   # classifier able to predict positives\n",
        "        data['fp'] = 45\n",
        "        data['tn'] = 5    # classifier was unable to predict negatives\n",
        "        data['fn'] = 3\n",
        "        # F1 measures a good score, MCC doesn't\n",
        "        # F1 0.66    MCC 0.07\n",
        "        f1_ = round(f1(data), 2)\n",
        "        mcc_ = round(mcc(data), 2)\n",
        "        self.assertEqual(f1_, 0.66)\n",
        "        self.assertEqual(mcc_, 0.07)\n",
        "\n",
        "        # Use Case B2: Balanced dataset\n",
        "        data = {}\n",
        "        data['ap'] = 50   # 50 positives\n",
        "        data['an'] = 50   # 50 negatives\n",
        "        data['se'] = 14\n",
        "        data['tp'] = 10   # classifier was unable to predict positives\n",
        "        data['fp'] = 4\n",
        "        data['tn'] = 46    # classifier able to predict negatives\n",
        "        data['fn'] = 40\n",
        "        # F1 measures a good score, MCC doesn't\n",
        "        # F1 0.31    MCC 0.17\n",
        "        f1_ = round(f1(data), 2)\n",
        "        mcc_ = round(mcc(data), 2)\n",
        "        self.assertEqual(f1_, 0.31)\n",
        "        self.assertEqual(mcc_, 0.17)\n",
        "\n",
        "        # Use Case C1: Negatively imbalanced dataset\n",
        "        data = {}\n",
        "        data['ap'] = 10   # 10 positives\n",
        "        data['an'] = 90   # 90 negatives\n",
        "        data['se'] = 98\n",
        "        data['tp'] = 9    # classifier was unable to predict positives\n",
        "        data['fp'] = 89\n",
        "        data['tn'] = 1    # classifier able to predict negatives\n",
        "        data['fn'] = 1\n",
        "        # Both the scores gives bad measure\n",
        "        # F1 0.17    MCC -0.19\n",
        "        f1_ = round(f1(data), 2)\n",
        "        mcc_ = round(mcc(data), 2)\n",
        "        self.assertEqual(f1_, 0.17)\n",
        "        self.assertEqual(mcc_, -0.19)\n",
        "\n",
        "        # Use Case C2: Negatively imbalanced dataset\n",
        "        data = {}\n",
        "        data['ap'] = 11   # 10 positives\n",
        "        data['an'] = 89   # 89 negatives\n",
        "        data['se'] = 3\n",
        "        data['tp'] = 2   # classifier was unable to predict positives\n",
        "        data['fp'] = 1\n",
        "        data['tn'] = 88    # classifier able to predict negatives\n",
        "        data['fn'] = 9\n",
        "        # Both the scores gives bad measure\n",
        "        # F1 0.29    MCC 0.31\n",
        "        f1_ = round(f1(data), 2)\n",
        "        mcc_ = round(mcc(data), 2)\n",
        "        self.assertEqual(f1_, 0.29)\n",
        "        self.assertEqual(mcc_, 0.31)\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bench.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}