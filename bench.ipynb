{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrH5gGJWRRLU"
   },
   "source": [
    "Drive linking to import needed folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjczG0hjRRWD",
    "outputId": "4588af6b-c5f6-4725-fa82-3ee5aea05af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lz9PBhYUNs0U",
    "outputId": "1eec7b7b-981a-46c2-c830-14625cc05cb4"
   },
   "outputs": [],
   "source": [
    "\n",
    "bench_mode = 'performance' # 'dataset' or 'performance'\n",
    "\n",
    "if bench_mode == 'dataset':\n",
    "    !rm -rf dataset\n",
    "\n",
    "    !unzip drive/MyDrive/testing/skinny/20210521-204405_p.zip -d dataset # skinny full\n",
    "    !mv dataset/20210521-204405/* dataset/skinny\n",
    "    !unzip drive/MyDrive/testing/skinny/20210523-192002_p.zip -d dataset # skinny skintones full\n",
    "    !mv dataset/20210523-192002/* dataset/skinny_st\n",
    "\n",
    "    !unzip drive/MyDrive/testing/bayes/bayes.zip -d dataset # bayes full\n",
    "    !unzip drive/MyDrive/testing/bayes/bayes_st.zip -d dataset # bayes skintones full\n",
    "\n",
    "    !unzip drive/MyDrive/testing/dyc/dyc.zip -d dataset # dyc\n",
    "    !unzip drive/MyDrive/testing/dyc/dyc_st.zip -d dataset # dyc\n",
    "elif bench_mode == 'performance':\n",
    "    !rm -rf performance\n",
    "\n",
    "    !unzip drive/MyDrive/testing/benchmark/bench_skinny.zip -d performance\n",
    "    !unzip drive/MyDrive/testing/benchmark/bench_bayes.zip -d performance\n",
    "    !unzip drive/MyDrive/testing/benchmark/bench_dyc.zip -d performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKBIxIogaNTv"
   },
   "source": [
    "Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li6dz5rB29EF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm # progress bar\n",
    "from PIL import Image # used as image loader\n",
    "import math\n",
    "import cv2 # image processing\n",
    "from statistics import mean, stdev, pstdev\n",
    "\n",
    "\n",
    "# Metric Utils\n",
    "\n",
    "\n",
    "\n",
    "# load images as numpy BOOL arrays (0-1)\n",
    "def load_images(gt_path: str, pred_path: str, threshold: int = 128):\n",
    "    # load as grayscale uint8\n",
    "    gt_gray = np.array(Image.open(gt_path).convert('L')) # .convert('LA')\n",
    "    pred_gray = np.array(Image.open(pred_path).convert('L')) #.convert('LA')\n",
    "    # binarize and convert to bool\n",
    "    gt_bool = gt_gray > threshold\n",
    "    pred_bool = pred_gray > threshold\n",
    "\n",
    "    # debug\n",
    "    # print(f'Dims: gt-gray={gt_gray.shape} gt-bool={gt_bool.shape} p-gray={pred_gray.shape} p-bool={pred_bool.shape}')\n",
    "    # display(Image.open(gt_path))\n",
    "    # display(Image.open(gt_path).convert('L'))\n",
    "    return gt_bool, pred_bool\n",
    "\n",
    "\n",
    "# OLD AVERAGE: calculate average between all experiments of EVERY metric\n",
    "# y and p files must have the same filename\n",
    "def pd_metrics_old(gt_dir: str, pred_dir: str, metric_fns: list, threshold: int = 128) -> list:\n",
    "    out = []\n",
    "\n",
    "    for y_filename in tqdm(os.listdir(gt_dir)):\n",
    "        y_path = os.path.join(gt_dir, y_filename)\n",
    "        p_filename = os.path.splitext(y_filename)[0] + '.png'\n",
    "        p_path = os.path.join(pred_dir, p_filename) # pred are always PNG\n",
    "\n",
    "\n",
    "        # start adding item data into a structure\n",
    "        idata = {}\n",
    "        idata['y'] = y_path\n",
    "        idata['p'] = p_path\n",
    "\n",
    "        # load images from paths and apply threshold to binarize\n",
    "        # the skin probabilities of predictions\n",
    "        y_true, y_pred = load_images(y_path, p_path, threshold)\n",
    "        confmat = confmat_scores(y_true, y_pred)\n",
    "\n",
    "        # calculate metrics for the image couple loaded\n",
    "        for metric_fn in metric_fns:\n",
    "            f_name = metric_fn.__name__\n",
    "            f_argcount = metric_fn.__code__.co_argcount # amount of argument in function definition\n",
    "\n",
    "            # only one args: the metric only uses confusion matrix scores and is LUT-optimized\n",
    "            if f_argcount == 1:\n",
    "                idata[f_name] = metric_fn(confmat)\n",
    "            # two args: confusion matrix scores aren't enough\n",
    "            else:\n",
    "                idata[f_name] = metric_fn(y_true, y_pred)\n",
    "\n",
    "            # debug\n",
    "            #display(Image.fromarray(y_true))\n",
    "            #display(Image.fromarray(y_pred))\n",
    "        \n",
    "        # update the final data list\n",
    "        out.append(idata)\n",
    "    \n",
    "    print(f'  Found {len(out)} matches')\n",
    "    \n",
    "    return out\n",
    "\n",
    "# print human-readable metrics results data\n",
    "# \n",
    "# !!NOTE!!\n",
    "# the mean F1 value calculated by summing all experiments F1 and divinding by N elements\n",
    "# is different than the mean calculated by applying the F1 formula on average REcall and PRecision\n",
    "def print_pd_mean_old(total: list, metric_fns: list, desc: str, method: str) -> None:\n",
    "    # prepare the JSON table\n",
    "    # JSON table item example: {\"name\":\"ecu\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"}\n",
    "    #{\"method\":\"Skinny\", \"train\":\"ecu\", \"test\":\"hgr\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"},\n",
    "    #{\"method\":\"Skinny\", \"train\":\"ecu\", \"test\":\"schmugge\", \"F1\":\".5123 +- 0.15\", \"IOU\":\".1744 +- 0.13\"},\n",
    "    #{\"method\":\"Bayes\", \"train\":\"ecu\", \"test\":\"hgr\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"}\n",
    "    res = {}\n",
    "    res['method'] = method\n",
    "    desc = os.path.basename(desc)\n",
    "    desc = desc.lower().replace('_small', '')\n",
    "    dss = desc.split('_')\n",
    "    ds_tr = dss[0]\n",
    "    ds_te = dss[2]\n",
    "    res['train'] = ds_tr\n",
    "    res['test'] = ds_te\n",
    "\n",
    "    print(f'{method}: {desc}') # output info\n",
    "\n",
    "    for metric_fn in metric_fns:\n",
    "        f_name = metric_fn.__name__\n",
    "        f_data = [ d[f_name] for d in total ]\n",
    "        #f_mean = sum(f_data) / len(total)\n",
    "        f_mean = mean(f_data)\n",
    "        #f_mean = round(f_mean, 4) # round to 4 decimals\n",
    "        f_mean = '{:.4f}'.format(f_mean) # round and zerofill\n",
    "        f_std = pstdev(f_data)\n",
    "        #f_std = round(f_std, 2) # round to 2 decimals\n",
    "        f_std = '{:.2f}'.format(f_std) # round and zerofill\n",
    "        print(f'{f_name}: {f_mean} ± {f_std}')\n",
    "\n",
    "        # add each metric data to the JSON table\n",
    "        res[f_name] = f'{f_mean} ± {f_std}'\n",
    "    \n",
    "    return res # return JSON table\n",
    "\n",
    "\n",
    "# NEW AVERAGE: calculate average only of medium-scores (PRecision, REcall, SPecificity)\n",
    "# y and p files must have the same filename\n",
    "def pd_metrics(gt_dir: str, pred_dir: str, metric_fns: list, threshold: int = 128) -> list:\n",
    "    out = []\n",
    "\n",
    "    for y_filename in tqdm(os.listdir(gt_dir)):\n",
    "        y_path = os.path.join(gt_dir, y_filename)\n",
    "        p_filename = os.path.splitext(y_filename)[0] + '.png'\n",
    "        p_path = os.path.join(pred_dir, p_filename) # pred are always PNG\n",
    "\n",
    "\n",
    "        # start adding item data into a structure\n",
    "        idata = {}\n",
    "        idata['y'] = y_path\n",
    "        idata['p'] = p_path\n",
    "\n",
    "        # load images from paths and apply threshold to binarize\n",
    "        # the skin probabilities of predictions\n",
    "        y_true, y_pred = load_images(y_path, p_path, threshold)\n",
    "        confmat = confmat_scores(y_true, y_pred)\n",
    "\n",
    "        # calculate metrics for the image couple loaded\n",
    "        for metric_fn in metric_fns:\n",
    "            f_name = metric_fn.__name__\n",
    "            f_argcount = metric_fn.__code__.co_argcount # amount of argument in function definition\n",
    "\n",
    "            if len(f_name.split('_')) > 1: # is a new-average metric, must not compute now\n",
    "                continue\n",
    "\n",
    "            # only one args: the metric only uses confusion matrix scores and is LUT-optimized\n",
    "            if f_argcount == 1:\n",
    "                idata[f_name] = metric_fn(confmat)\n",
    "            # two args: confusion matrix scores aren't enough\n",
    "            else:\n",
    "                idata[f_name] = metric_fn(y_true, y_pred)\n",
    "\n",
    "            # debug\n",
    "            #display(Image.fromarray(y_true))\n",
    "            #display(Image.fromarray(y_pred))\n",
    "        \n",
    "        # update the final data list\n",
    "        out.append(idata)\n",
    "    \n",
    "    print(f'  Found {len(out)} matches')\n",
    "    \n",
    "    return out\n",
    "\n",
    "# print human-readable metrics results data\n",
    "# \n",
    "# !!NOTE!!\n",
    "# the mean F1 value calculated by summing all experiments F1 and divinding by N elements\n",
    "# is different than the mean calculated by applying the F1 formula on average REcall and PRecision\n",
    "def print_pd_mean(total: list, metric_fns: list, desc: str) -> None:\n",
    "    print(f'{desc}')\n",
    "    res = {}\n",
    "    new_avg = []\n",
    "\n",
    "    for metric_fn in metric_fns:\n",
    "        f_name = metric_fn.__name__\n",
    "        f_score = -99\n",
    "\n",
    "        if len(f_name.split('_')) > 1: # is a new-average metric\n",
    "            new_avg.append(metric_fn)\n",
    "            continue\n",
    "        else:\n",
    "            f_score = sum(d[f_name] for d in total) / len(total)\n",
    "            res[f_name] = f_score\n",
    "    \n",
    "    for metric_fn in new_avg:\n",
    "        f_name = metric_fn.__name__.split('_')[0]\n",
    "        f_score = metric_fn(res['precision'], res['recall'], res['specificity'])\n",
    "        res[f_name] = f_score\n",
    "    \n",
    "    for key, value in res.items():\n",
    "        value = round(value, 4) # round to 4 decimals\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "\n",
    "# ALT AVERAGE: calculate average only of primitive confmat scores!\n",
    "# y and p files must have the same filename\n",
    "def pd_metrics_a(gt_dir: str, pred_dir: str, threshold: int = 128) -> list:\n",
    "    out = []\n",
    "\n",
    "    for y_filename in tqdm(os.listdir(gt_dir)):\n",
    "        y_path = os.path.join(gt_dir, y_filename)\n",
    "        p_filename = os.path.splitext(y_filename)[0] + '.png'\n",
    "        p_path = os.path.join(pred_dir, p_filename) # pred are always PNG\n",
    "\n",
    "        # load images from paths and apply threshold to binarize\n",
    "        # the skin probabilities of predictions\n",
    "        y_true, y_pred = load_images(y_path, p_path, threshold)\n",
    "        confmat = confmat_scores(y_true, y_pred)\n",
    "        # append some metadata to identify the images\n",
    "        confmat['!y'] = y_path\n",
    "        confmat['!p'] = p_path\n",
    "        \n",
    "        # update the final data list\n",
    "        out.append(confmat)\n",
    "    \n",
    "    print(f'  Found {len(out)} matches')\n",
    "    \n",
    "    return out\n",
    "\n",
    "# print human-readable metrics results data\n",
    "# \n",
    "# !!NOTE!!\n",
    "# the mean F1 value calculated by summing all experiments F1 and divinding by N elements\n",
    "# is different than the mean calculated by applying the F1 formula on average REcall and PRecision\n",
    "def print_pd_mean_aa(total: list, metric_fns: list, desc: str) -> None:\n",
    "    print(f'{desc}')\n",
    "    confmat_avg = {}\n",
    "\n",
    "    # Calculate average confmat scores\n",
    "    confmat_keys = total[0].keys()\n",
    "    for confmat_key in confmat_keys:\n",
    "        if not confmat_key.startswith('!'): # a key that starts with '!' it's metadata, not a metric\n",
    "            confmat_key_avg = sum(d[confmat_key] for d in total) / len(total)\n",
    "            confmat_avg[confmat_key] = confmat_key_avg\n",
    "            #print(f'{confmat_key}: {sum(d[confmat_key] for d in total) / len(total)}')\n",
    "    \n",
    "    # Use average confmat scores to calculate given metrics\n",
    "    for metric_fn in metric_fns:\n",
    "        f_name = metric_fn.__name__ # get metric (function) name\n",
    "        f_score = metric_fn(confmat_avg)\n",
    "        print(f'{f_name}: {f_score}')\n",
    "\n",
    "# print human-readable metrics results data\n",
    "# \n",
    "# !!NOTE!!\n",
    "# the mean F1 value calculated by summing all experiments F1 and divinding by N elements\n",
    "# is different than the mean calculated by applying the F1 formula on average REcall and PRecision\n",
    "def print_pd_mean_a(total: list, metric_fns: list, desc: str) -> None:\n",
    "    print(f'{desc}')\n",
    "    confmat_avg = {}\n",
    "\n",
    "    # Calculate average confmat scores\n",
    "    confmat_keys = total[0].keys()\n",
    "    for confmat_key in confmat_keys:\n",
    "        if not confmat_key.startswith('!'): # a key that starts with '!' it's metadata, not a metric\n",
    "            confmat_avg[confmat_key] = sum(d[confmat_key] for d in total)\n",
    "            #print(f'{confmat_key}: {sum(d[confmat_key] for d in total) / len(total)}')\n",
    "    \n",
    "    # Use average confmat scores to calculate given metrics\n",
    "    for metric_fn in metric_fns:\n",
    "        f_name = metric_fn.__name__ # get metric (function) name\n",
    "        f_score = metric_fn(confmat_avg)\n",
    "        print(f'{f_name}: {f_score}')\n",
    "\n",
    "\n",
    "\n",
    "# Dataset converting Utils\n",
    "\n",
    "# convert binary like GT in the VDM GT format (red where there is skin overlayed the ORI images)\n",
    "# (and updates CSV file with the new gt path)\n",
    "def bin2vdm(csv_file, out_dir):\n",
    "    # read the images CSV\n",
    "    file = open(csv_file)\n",
    "    file3c = file.read().splitlines()\n",
    "    file.close()\n",
    "\n",
    "    # rewrite csv file\n",
    "    with open(csv_file, 'w') as out:\n",
    "        for entry in file3c:\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            gt_path = entry.split(csv_sep)[1]\n",
    "            \n",
    "\n",
    "            # process images\n",
    "            # load images\n",
    "            ori_im = cv2.imread(ori_path)\n",
    "            gt_im = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # everything but skin\n",
    "            butsk = cv2.copyTo(ori_im, cv2.bitwise_not(gt_im))\n",
    "            # splitto il risultato\n",
    "            b,g,r = cv2.split(butsk)\n",
    "            # utilizzo la maschera come canale red (rossa al posto che bianca)\n",
    "            r = cv2.bitwise_or(r, gt_im)\n",
    "            # riunisco gli split con il nuovo canale red\n",
    "            res = cv2.merge([b,g,r])\n",
    "\n",
    "            cv2.imwrite(out_dir, res)\n",
    "            # update gt path\n",
    "            gt_path = out_dir\n",
    "\n",
    "            note = entry.split(csv_sep)[2]\n",
    "            skintone = ''\n",
    "            if len(entry.split(csv_sep)) == 4:\n",
    "                skintone = csv_sep + entry.split(csv_sep)[3]\n",
    "            \n",
    "            # aggiorno CSV\n",
    "            out.write(f\"{ori_path}{csv_sep}{gt_path}{csv_sep}{note}{skintone}\\n\")\n",
    "\n",
    "# convert SDDMA output masks (green where skin is detected overlayed to the ORI image)\n",
    "# to binary GT\n",
    "def sddmaout2bin(in_dir, out_dir):\n",
    "    # loop mask files\n",
    "    for im_basename in os.listdir(in_dir):\n",
    "        im_path = os.path.join(in_dir, im_basename)\n",
    "        \n",
    "        # controlla se e' un'immagine (per evitare problemi con files come thumbs.db)\n",
    "        if not os.path.isdir(im_path) and imghdr.what(im_path) != None:\n",
    "            im = cv2.imread(im_path)\n",
    "\n",
    "            b = 0\n",
    "            g = 255\n",
    "            r = 0\n",
    "            lower_val = (b, g, r)\n",
    "            upper_val = lower_val\n",
    "            # Threshold the image to get only selected colors\n",
    "            mask = cv2.inRange(im, lower_val, upper_val)\n",
    "            cv2.imwrite(out_dir, mask)\n",
    "\n",
    "def is_better(value1, value2, mode: str):\n",
    "    if mode == 'upper':\n",
    "        return value1 > value2\n",
    "    else:\n",
    "        return value1 < value2\n",
    "\n",
    "def bold_best(data: list, datas: list, base = False):\n",
    "    maxv = {}\n",
    "    # save the best values between the METHODS (skinny/bayes) for each metric and dataset combination\n",
    "    for obj in data:\n",
    "        o_m = obj['method']\n",
    "        o_train = obj['train']\n",
    "        o_test = obj['test']\n",
    "        o_f1 = obj['f1']\n",
    "        o_iou = obj['iou']\n",
    "        o_dprs = obj['dprs']\n",
    "        f1iou = float(o_f1.split(' ')[0]) - float(o_iou.split(' ')[0])\n",
    "        obj['f1iou'] = round(f1iou, 4)\n",
    "\n",
    "        print(obj)\n",
    "\n",
    "        # they are cross predictions hence test != train\n",
    "        if base == False and o_train == o_test:\n",
    "            continue\n",
    "        \n",
    "        # for each table metric multirow group\n",
    "        for f_name in ['f1', 'iou', 'dprs', 'f1iou']:\n",
    "            fn_val = obj[f_name] # metric value of the current iteration\n",
    "            fn_idformat = f'{f_name}_{o_train}_{o_test}' # ID format\n",
    "            fnv = f'{fn_idformat}_v' # value of the max measurement\n",
    "            fni = f'{fn_idformat}_i' # ID of the max measurement\n",
    "\n",
    "            bmode = 'upper'\n",
    "            if f_name == 'dprs' or f_name == 'f1iou':\n",
    "                bmode = 'lower'\n",
    "\n",
    "            # for each table column\n",
    "            for trdata in datas:\n",
    "                for tedata in datas:\n",
    "                    if o_train == trdata and o_test == tedata:\n",
    "                        # save best between methods\n",
    "\n",
    "                        # if max does not exist, add its value and ID\n",
    "                        if fnv not in maxv:\n",
    "                            maxv[fnv] =  fn_val\n",
    "                            maxv[fni] =  o_m\n",
    "                        # if new max, save the measurement and its ID\n",
    "                        elif f_name != 'f1iou' and is_better(float(fn_val.split(' ')[0]), float(maxv[fnv].split(' ')[0]), bmode):\n",
    "                            maxv[fnv] =  fn_val\n",
    "                            maxv[fni] =  o_m\n",
    "                        elif f_name == 'f1iou':\n",
    "                            if is_better(float(fn_val), float(maxv[fnv]), bmode):\n",
    "                                maxv[fnv] =  fn_val\n",
    "                                maxv[fni] =  o_m\n",
    "    newdata = []\n",
    "    # and now make them bold\n",
    "    for obj in data:\n",
    "        o_m = obj['method']\n",
    "        o_train = obj['train']\n",
    "        o_test = obj['test']\n",
    "        o_f1 = obj['f1']\n",
    "        o_iou = obj['iou']\n",
    "        o_dprs = obj['dprs']\n",
    "        f1iou = float(o_f1.split(' ')[0]) - float(o_iou.split(' ')[0])\n",
    "        obj['f1iou'] = '{:.4f}'.format(f1iou) # round and zerofill\n",
    "\n",
    "        for f_name in ['f1', 'iou', 'dprs', 'f1iou']:\n",
    "            fn_val = obj[f_name] # metric value of the current iteration\n",
    "            fn_idformat = f'{f_name}_{o_train}_{o_test}' # ID format\n",
    "            fnv = f'{fn_idformat}_v' # value of the max measurement\n",
    "            fni = f'{fn_idformat}_i' # ID of the max measurement\n",
    "\n",
    "            #if maxv[fni] == fn_idformat:\n",
    "            if fni in maxv and maxv[fni] == o_m:\n",
    "                obj_formatted = '\\\\texttt{' + '\\\\textbf{' + str(obj[f_name]) + '}' + '}'\n",
    "                obj[f_name] = obj_formatted # set bold and monospace\n",
    "                newdata.append(obj)\n",
    "            else:\n",
    "                obj_formatted = '\\\\texttt{' + str(obj[f_name]) + '}'\n",
    "                obj[f_name] = obj_formatted # set monospace\n",
    "                newdata.append(obj)\n",
    "    data = newdata\n",
    "    print('newdata:')\n",
    "    print(data)\n",
    "\n",
    "    # change JSON format into a standalone data structure containing all table variables\n",
    "    ff = {}\n",
    "    for obj in data:\n",
    "        o_m = obj['method']\n",
    "        o_train = obj['train']\n",
    "        o_test = obj['test']\n",
    "        o_f1 = obj['f1']\n",
    "        o_iou = obj['iou']\n",
    "        o_dprs = obj['dprs']\n",
    "        f1iou = obj['f1iou']\n",
    "\n",
    "        ff[f'f1_{o_m}_{o_train}_{o_test}'] = o_f1\n",
    "        ff[f'iou_{o_m}_{o_train}_{o_test}'] = o_iou\n",
    "        ff[f'dprs_{o_m}_{o_train}_{o_test}'] = o_dprs\n",
    "        ff[f'f1iou_{o_m}_{o_train}_{o_test}'] = f1iou\n",
    "    \n",
    "    print(ff)\n",
    "    return ff\n",
    "\n",
    "# data is a list of JSON items\n",
    "# JSON item example: {\"name\":\"ecu\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"}\n",
    "def get_latex_cross(data: list, datas = None):\n",
    "    tex_body = ''\n",
    "\n",
    "    if datas == None:\n",
    "        datas = ['ecu', 'hgr', 'schmugge']\n",
    "\n",
    "    ff = bold_best(data, datas)\n",
    "\n",
    "    # start building the body string\n",
    "    tex_ms = ['F1 $\\\\uparrow$', 'IOU $\\\\uparrow$', 'Dprs $\\\\downarrow$', 'F1 - IOU $\\\\downarrow$']\n",
    "    i = 2\n",
    "    for tm in tex_ms:\n",
    "        mns = tm.split(' ')\n",
    "\n",
    "        if len(mns) > 2: # f1 - iou\n",
    "            mn = 'f1iou'\n",
    "        else:\n",
    "            mn = mns[0].lower()\n",
    "        \n",
    "        # for each metrics there are 2 lines(methods): Skinny and Bayes\n",
    "        for j in range(2):\n",
    "            pfix = ''\n",
    "\n",
    "            if j == 0:\n",
    "                met = 'skinny'\n",
    "                mf = met[0].lower()\n",
    "                metf = met.upper() + '\\\\rule{0pt}{14pt}' # spacing between multirows (metrics)\n",
    "                metric_w_arrow = tm\n",
    "                pfix = f'''\\\\multirow{{2}}{{*}}{{{{{metric_w_arrow}}}}}'''\n",
    "            elif j == 1:\n",
    "                met = 'bayes'\n",
    "                mf = met[0].lower()\n",
    "                metf = met.upper()\n",
    "            \n",
    "            if datas != ['ecu', 'hgr', 'schmugge']:\n",
    "                met = f'{met}_st'\n",
    "\n",
    "            # another data struct to gather all items necessary for writing a table line\n",
    "            tmp = {}\n",
    "            datas_startletter = []\n",
    "            for ds_tr in datas:\n",
    "                datas_startletter.append(ds_tr[0].lower())\n",
    "                for ds_te in datas:\n",
    "                    if ds_tr == ds_te:\n",
    "                        continue\n",
    "                    tmp[f'{mf}_{ds_tr[0].lower()}{ds_te[0].lower()}'] = ff[f'{mn}_{met}_{ds_tr}_{ds_te}']\n",
    "\n",
    "            tex_body += f'{pfix}& {metf}'\n",
    "            for letter_tr in datas_startletter:\n",
    "                for letter_te in datas_startletter:\n",
    "                    if letter_tr != letter_te:\n",
    "                        table_item = tmp[f'{mf}_{letter_tr}{letter_te}']\n",
    "                        tex_body += f' & {table_item}'\n",
    "            tex_body += '\\\\\\\\'\n",
    "    \n",
    "    # string header\n",
    "    tex_header = r'''\n",
    "    \\begin{tabular}{clcccccc}\n",
    "    \\toprule\n",
    "    \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Training}} \n",
    "    '''\n",
    "    \n",
    "    # add first row\n",
    "    for dss in datas:\n",
    "        tex_header += r'& \\multicolumn{2}{c}{\\head{' + dss.upper() + '}} '\n",
    "    tex_header += r'\\\\'\n",
    "\n",
    "    tex_header += r'\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Testing}} '\n",
    "    # add second row\n",
    "    for dss in datas:\n",
    "        for dssd in datas:\n",
    "            if dssd != dss:\n",
    "                tex_header += r' & \\multicolumn{1}{c}{\\head{' + dssd.upper() + '}} '\n",
    "    tex_header += r'\\\\'\n",
    "    tex_header += r'\\midrule'\n",
    "\n",
    "    # string end\n",
    "    tex_end = r'''\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    '''\n",
    "\n",
    "    tex = tex_header + tex_body + tex_end\n",
    "    return tex\n",
    "\n",
    "# data is a list of JSON items\n",
    "# JSON item example: {\"name\":\"ecu\", \"F1\":\".9123 +- 0.25\", \"IOU\":\".8744 +- 0.11\"}\n",
    "def get_latex_base(data: list, datas = None):\n",
    "    tex_body = ''\n",
    "\n",
    "    if datas == None:\n",
    "        datas = ['ecu', 'hgr', 'schmugge']\n",
    "\n",
    "    ff = bold_best(data, datas, True)\n",
    "\n",
    "    # start building the body string\n",
    "    metrics = ['f1', 'iou', 'dprs']\n",
    "    \n",
    "    # first loop ROWS\n",
    "    # for each metrics there are 2 lines(methods): Skinny and Bayes, DYC\n",
    "    for j in range(3):\n",
    "        if j == 0:\n",
    "            met = 'skinny'\n",
    "        elif j == 1:\n",
    "            met = 'bayes'\n",
    "        else:\n",
    "            met = 'dyc'\n",
    "        metf = met.upper()\n",
    "        mf = met[0].lower()\n",
    "\n",
    "        if datas != ['ecu', 'hgr', 'schmugge']:\n",
    "            met = f'{met}_st'\n",
    "\n",
    "        # another data struct to gather all items necessary for writing a table line\n",
    "        tmp = {}\n",
    "        # then loop COLUMNS\n",
    "        datas_startletter = []\n",
    "        for ds in datas:\n",
    "            datas_startletter.append(ds[0].lower())\n",
    "            for tm in metrics:\n",
    "                tmp[f'{mf}_{ds[0].lower()}{tm}'] = ff[f'{tm}_{met}_{ds}_{ds}']\n",
    "\n",
    "        # m is method\n",
    "        # eh = ecu_on_hgr, es = ecu_on_schmugge, ...\n",
    "        tex_body += f'{metf}'\n",
    "        for letter in datas_startletter:\n",
    "            for tmm in metrics:\n",
    "                table_item = tmp[f'{mf}_{letter}{tmm}']\n",
    "                tex_body += f' & {table_item}'\n",
    "        tex_body += '\\\\\\\\'\n",
    "    \n",
    "    # string header\n",
    "    tex_header = r'''\n",
    "    \\begin{tabular}{lccccccccc}\n",
    "    \\toprule\n",
    "    '''\n",
    "    \n",
    "    # add first row\n",
    "    for dss in datas:\n",
    "        tex_header += r'& \\multicolumn{3}{c}{\\head{' + dss.upper() + '}} '\n",
    "    tex_header += r'\\\\'\n",
    "    \n",
    "    tex_header += r'''\n",
    "    & \\multicolumn{1}{c}{\\head{F1 $\\uparrow$}} & \\multicolumn{1}{c}{\\head{IOU $\\uparrow$}} & \\multicolumn{1}{c}{\\head{Dprs $\\downarrow$}}\n",
    "    & \\multicolumn{1}{c}{\\head{F1 $\\uparrow$}} & \\multicolumn{1}{c}{\\head{IOU $\\uparrow$}} & \\multicolumn{1}{c}{\\head{Dprs $\\downarrow$}}\n",
    "    & \\multicolumn{1}{c}{\\head{F1 $\\uparrow$}} & \\multicolumn{1}{c}{\\head{IOU $\\uparrow$}} & \\multicolumn{1}{c}{\\head{Dprs $\\downarrow$}}\\\\\n",
    "    \\midrule\n",
    "    '''\n",
    "\n",
    "    # string end\n",
    "    tex_end = r'''\n",
    "    \\bottomrule\n",
    "    \\end{tabular}\n",
    "    '''\n",
    "\n",
    "    tex = tex_header + tex_body + tex_end\n",
    "    return tex\n",
    "\n",
    "def read_performance(perf_dir: str):\n",
    "    csv_sep = ','\n",
    "\n",
    "    # will contain the final mean between each observation's mean\n",
    "    observations_means = []\n",
    "\n",
    "    # do the mean of each observation\n",
    "    for i in range(5):\n",
    "        perf_filename = f'bench{i}.txt'\n",
    "        perf_file = os.path.join(perf_dir, perf_filename)\n",
    "\n",
    "        # read txt lines (as csv)\n",
    "        file2c = open(perf_file)\n",
    "        doubles = file2c.read().splitlines()\n",
    "        file2c.close()\n",
    "\n",
    "        intra_obs_timelist = []\n",
    "        for entry in doubles: # ori_path, execution_time(s)\n",
    "            ori_path = entry.split(csv_sep)[0]\n",
    "            execution_time = entry.split(csv_sep)[1]\n",
    "            intra_obs_timelist.append(float(execution_time))\n",
    "        \n",
    "        #print(execution_timelist)\n",
    "        obs_mean = mean(intra_obs_timelist)\n",
    "        obs_mean = '{:.6f}'.format(obs_mean) # round and zerofill\n",
    "        obs_std = pstdev(intra_obs_timelist)\n",
    "        obs_std = '{:.3f}'.format(obs_std) # round and zerofill\n",
    "\n",
    "        obs_string = f'{obs_mean} ± {obs_std}'\n",
    "\n",
    "        #execution_timelist_means.append(float(p_mean))\n",
    "        observations_means.append(obs_string)\n",
    "        print(f'{perf_dir} at {i}: {obs_string}')\n",
    "    \n",
    "    # get the means from observation means, without the std\n",
    "    obs_mean_values = []\n",
    "    for entry in observations_means:\n",
    "        obs_mean_values.append(float(entry.split(' ')[0]))\n",
    "    \n",
    "    # do the final mean of the observation means\n",
    "    fin_mean = mean(obs_mean_values)\n",
    "    fin_mean = '{:.6f}'.format(fin_mean) # round and zerofill\n",
    "    fin_std = pstdev(obs_mean_values)\n",
    "    fin_std = '{:.3f}'.format(fin_std) # round and zerofill\n",
    "\n",
    "    fin_string = f'{fin_mean} ± {fin_std}'\n",
    "\n",
    "    print(f'{perf_dir} at FIN: {fin_string}\\n')\n",
    "    return fin_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PL8XiiLuhDz"
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "# false negatives weight more (cannot be fixed by post-processing whereas false positives can to a degree),\n",
    "# choose metrics accordingly (maybe False negative rate?)\n",
    "#\n",
    "# The recall close to 1.0 effectively means false_negatives close to 0.0, which is what to want\n",
    "# (precision_recall_curve)\n",
    "#\n",
    "# Fb measure (F1 generalized that can be weighted more on recall or precision)\n",
    "# https://machinelearningmastery.com/fbeta-measure-for-machine-learning/\n",
    "\n",
    "\n",
    "# prevents zero division\n",
    "smooth = 1e-20 #1e-07\n",
    "\n",
    "\n",
    "# returns a dict that can be used as a LUT-table of the confusion matrix scores\n",
    "# for scores info, see quick graph https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "# dtype casting is used to prevent overflow long_scalars\n",
    "def confmat_scores(y_true, y_pred) -> dict:\n",
    "    data = {}\n",
    "    cast_type = 'double'\n",
    "\n",
    "    neg_y_true = 1 - y_true\n",
    "    neg_y_pred = 1 - y_pred\n",
    "\n",
    "    AP = np.sum(y_true, dtype=cast_type) # TP + FN\n",
    "    AN = np.sum(neg_y_true, dtype='double') # TN + FP\n",
    "    SE = np.sum(y_pred, dtype='double') #TP + FP\n",
    "    TP = np.sum(y_true * y_pred, dtype='double')\n",
    "    FP = SE - TP\n",
    "    TN = np.sum(neg_y_true * neg_y_pred, dtype='double')\n",
    "    FN = AP - TP\n",
    "\n",
    "    data['ap'] = AP\n",
    "    data['an'] = AN\n",
    "    data['se'] = SE\n",
    "    data['tp'] = TP\n",
    "    data['fp'] = FP\n",
    "    data['tn'] = TN\n",
    "    data['fn'] = FN\n",
    "\n",
    "    return data\n",
    "\n",
    "def iou_old(y_true, y_pred) -> float:\n",
    "    overlap = y_true*y_pred # Logical AND (or use np.logical_and(target, prediction))\n",
    "    union = y_true + y_pred # Logical OR (or use np.logical_or(target, prediction))\n",
    "\n",
    "    # (or iou_score = np.sum(intersection) / np.sum(union))\n",
    "    IOU = overlap.sum() / (union.sum() + smooth) # Treats \"True\" as 1,\n",
    "                                                      # sums number of Trues\n",
    "                                                      # in overlap and union\n",
    "                                                      # and divides\n",
    "    return IOU\n",
    "\n",
    "# Intersection over Union\n",
    "# can be re-expressed in terms of precision and recall\n",
    "# https://tomkwok.com/posts/iou-vs-f1/\n",
    "def iou(cs):\n",
    "    # precision_score = precision(cs)\n",
    "    # recall_score = recall(cs)\n",
    "    # return (precision_score * recall_score) / (precision_score + recall_score - precision_score * recall_score)\n",
    "    return cs['tp'] / (cs['tp'] + cs['fp'] + cs['fn'] + smooth)\n",
    "\n",
    "# Recall (aliases: TruePositiveRate, Sensitivity)\n",
    "# how many relevant items are selected?\n",
    "def recall(cs):\n",
    "    return cs['tp'] / (cs['ap'] + smooth)\n",
    "\n",
    "# Specificity (aliases: FalsePositiveRate)\n",
    "# how many negative elements are truly negative?\n",
    "def specificity(cs):\n",
    "    return cs['tn'] / (cs['an'] + smooth)\n",
    "\n",
    "# how many selected items are relevant?\n",
    "def precision(cs):\n",
    "    return cs['tp'] / (cs['se'] + smooth)\n",
    "\n",
    "# Fb-measure: recall is considered Beta(b) times important as precision\n",
    "# F2 weights recall higher than precision, F.5 weights precision higher than recall\n",
    "# Beta(b) is a positive real factor\n",
    "def fb(cs, b = 1):\n",
    "    precision_score = precision(cs)\n",
    "    recall_score = recall(cs)\n",
    "    return (1 + b**2) * ((precision_score * recall_score) / ((b**2 * precision_score) + recall_score + smooth))\n",
    "\n",
    "# F1-score (aliases: F1-measure, F-score with Beta=1)\n",
    "def f1(cs):\n",
    "    #precision_score = precision(cs)\n",
    "    #recall_score = recall(cs)\n",
    "    #return 2 * (float(precision_score * recall_score) / float(precision_score + recall_score + smooth))\n",
    "    return fb(cs)\n",
    "\n",
    "# F2-score\n",
    "def f2(cs):\n",
    "    return fb(cs, 2)\n",
    "\n",
    "def dprs(cs):\n",
    "    a = (1 - precision(cs))**2\n",
    "    b = (1 - recall(cs))**2\n",
    "    c = (1 - specificity(cs))**2\n",
    "    \n",
    "    return math.sqrt(a + b + c)\n",
    "\n",
    "def f1_n(pr, re, sp):\n",
    "    return 2 * ((pr * re) / (pr + re + smooth))\n",
    "\n",
    "def dprs_n(pr, re, sp):\n",
    "    a = (1 - pr)**2\n",
    "    b = (1 - re)**2\n",
    "    c = (1 - sp)**2\n",
    "    return math.sqrt(a + b + c)\n",
    "\n",
    "# range is [-1 1]\n",
    "def mcc(cs):\n",
    "    # explained in https://doi.org/10.1186/s12864-019-6413-7\n",
    "    # the following fixes prevent where MCC could not be calculated normally\n",
    "    M = np.matrix([[cs['tp'], cs['fn']], [cs['fp'], cs['tn']]]) # define confusion matrix\n",
    "    nz = np.count_nonzero(M) # get non-zero elements of the matrix\n",
    "    # fix 1\n",
    "    if nz == 1: # 3 elements of M are 0\n",
    "        # all samples of the dataset belong to 1 class\n",
    "        if cs['tp'] != 0 or cs['tn'] != 0: # they either are all correctly classified\n",
    "            return 1\n",
    "        else:\n",
    "            return -1 # or all uncorrectly classified\n",
    "    \n",
    "    # fix 2\n",
    "    # where a row or a column of M are zero while the other true entries\n",
    "    # are non zero, MCC takes the indefinite form 0/0\n",
    "    if nz == 2 and np.sum(np.abs(M.diagonal())) != 0 and np.sum(np.abs(np.diag(np.fliplr(M)))) != 0:\n",
    "        # replace the zero elements with an arbitrary small value \n",
    "        M[M == 0] = smooth\n",
    "    \n",
    "    # calculate MCC\n",
    "    num = cs['tp'] * cs['tn'] - cs['fp'] * cs['fn']\n",
    "    den = math.sqrt((cs['tp'] + cs['fp']) * (cs['tp'] + cs['fn']) * (cs['tn'] + cs['fp']) * (cs['tn'] + cs['fn']))\n",
    "\n",
    "    # print(f'num={num} den={den} TP={TP} FP={FP} FN={FN} TN={TN}') # debug\n",
    "\n",
    "    return num / (den + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDs8NXDGdKGV",
    "outputId": "2b606c6d-7dc4-4cec-99a0-389de2ffd81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance/bench_skinny at 0: 0.807026 ± 0.064\n",
      "performance/bench_skinny at 1: 0.797650 ± 0.008\n",
      "performance/bench_skinny at 2: 0.811103 ± 0.010\n",
      "performance/bench_skinny at 3: 0.911956 ± 0.242\n",
      "performance/bench_skinny at 4: 0.805169 ± 0.008\n",
      "performance/bench_skinny at FIN: 0.826581 ± 0.043\n",
      "\n",
      "performance/bench_bayes at 0: 0.459174 ± 0.001\n",
      "performance/bench_bayes at 1: 0.457149 ± 0.003\n",
      "performance/bench_bayes at 2: 0.458998 ± 0.002\n",
      "performance/bench_bayes at 3: 0.458094 ± 0.001\n",
      "performance/bench_bayes at 4: 0.454253 ± 0.002\n",
      "performance/bench_bayes at FIN: 0.457534 ± 0.002\n",
      "\n",
      "performance/bench_dyc at 0: 0.007665 ± 0.000\n",
      "performance/bench_dyc at 1: 0.007677 ± 0.000\n",
      "performance/bench_dyc at 2: 0.007730 ± 0.000\n",
      "performance/bench_dyc at 3: 0.007763 ± 0.000\n",
      "performance/bench_dyc at 4: 0.007752 ± 0.000\n",
      "performance/bench_dyc at FIN: 0.007717 ± 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if bench_mode == 'performance':\n",
    "    detectors = ['skinny', 'bayes', 'dyc']\n",
    "\n",
    "    for det in detectors:\n",
    "        perf_dir = f'performance/bench_{det}'\n",
    "        read_performance(perf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IebRxrN4ZpQX",
    "outputId": "c4bf0a26-b6ca-4dae-91fa-c93534568204"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 476.37it/s]\n",
      " 56%|█████▋    | 57/101 [00:00<00:00, 567.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 101 matches\n",
      "skinny_st: dark_on_medium\n",
      "f1: 0.7300 ± 0.25\n",
      "iou: 0.6279 ± 0.27\n",
      "dprs: 0.3805 ± 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 412.20it/s]\n",
      " 13%|█▎        | 53/409 [00:00<00:00, 529.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 101 matches\n",
      "bayes_st: dark_on_medium\n",
      "f1: 0.7928 ± 0.11\n",
      "iou: 0.6668 ± 0.11\n",
      "dprs: 0.3481 ± 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:00<00:00, 514.25it/s]\n",
      "  8%|▊         | 31/409 [00:00<00:01, 303.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 409 matches\n",
      "skinny_st: dark_on_light\n",
      "f1: 0.7262 ± 0.26\n",
      "iou: 0.6276 ± 0.28\n",
      "dprs: 0.3934 ± 0.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:00<00:00, 438.01it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 506.75it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 231.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 409 matches\n",
      "bayes_st: dark_on_light\n",
      "f1: 0.7577 ± 0.12\n",
      "iou: 0.6229 ± 0.13\n",
      "dprs: 0.4679 ± 0.18\n",
      "  Found 27 matches\n",
      "skinny_st: medium_on_dark\n",
      "f1: 0.8447 ± 0.13\n",
      "iou: 0.7486 ± 0.15\n",
      "dprs: 0.2326 ± 0.17\n",
      "  Found 27 matches\n",
      "bayes_st: medium_on_dark\n",
      "f1: 0.5628 ± 0.14\n",
      "iou: 0.4042 ± 0.13\n",
      "dprs: 0.6802 ± 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:00<00:00, 519.37it/s]\n",
      "  8%|▊         | 31/409 [00:00<00:01, 306.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 409 matches\n",
      "skinny_st: medium_on_light\n",
      "f1: 0.8904 ± 0.14\n",
      "iou: 0.8214 ± 0.16\n",
      "dprs: 0.1692 ± 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:00<00:00, 453.80it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 555.22it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 251.33it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 409 matches\n",
      "bayes_st: medium_on_light\n",
      "f1: 0.7032 ± 0.14\n",
      "iou: 0.5571 ± 0.14\n",
      "dprs: 0.5376 ± 0.23\n",
      "  Found 27 matches\n",
      "skinny_st: light_on_dark\n",
      "f1: 0.7660 ± 0.17\n",
      "iou: 0.6496 ± 0.21\n",
      "dprs: 0.3402 ± 0.21\n",
      "  Found 27 matches\n",
      "bayes_st: light_on_dark\n",
      "f1: 0.5293 ± 0.20\n",
      "iou: 0.3852 ± 0.19\n",
      "dprs: 0.6361 ± 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 511.69it/s]\n",
      " 58%|█████▊    | 59/101 [00:00<00:00, 584.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 101 matches\n",
      "skinny_st: light_on_medium\n",
      "f1: 0.9229 ± 0.11\n",
      "iou: 0.8705 ± 0.13\n",
      "dprs: 0.1192 ± 0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:00<00:00, 434.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 101 matches\n",
      "bayes_st: light_on_medium\n",
      "f1: 0.7853 ± 0.11\n",
      "iou: 0.6574 ± 0.12\n",
      "dprs: 0.3199 ± 0.16\n",
      "{'method': 'skinny_st', 'train': 'dark', 'test': 'medium', 'f1': '0.7300 ± 0.25', 'iou': '0.6279 ± 0.27', 'dprs': '0.3805 ± 0.33', 'f1iou': 0.1021}\n",
      "{'method': 'bayes_st', 'train': 'dark', 'test': 'medium', 'f1': '0.7928 ± 0.11', 'iou': '0.6668 ± 0.11', 'dprs': '0.3481 ± 0.16', 'f1iou': 0.126}\n",
      "{'method': 'skinny_st', 'train': 'dark', 'test': 'light', 'f1': '0.7262 ± 0.26', 'iou': '0.6276 ± 0.28', 'dprs': '0.3934 ± 0.34', 'f1iou': 0.0986}\n",
      "{'method': 'bayes_st', 'train': 'dark', 'test': 'light', 'f1': '0.7577 ± 0.12', 'iou': '0.6229 ± 0.13', 'dprs': '0.4679 ± 0.18', 'f1iou': 0.1348}\n",
      "{'method': 'skinny_st', 'train': 'medium', 'test': 'dark', 'f1': '0.8447 ± 0.13', 'iou': '0.7486 ± 0.15', 'dprs': '0.2326 ± 0.17', 'f1iou': 0.0961}\n",
      "{'method': 'bayes_st', 'train': 'medium', 'test': 'dark', 'f1': '0.5628 ± 0.14', 'iou': '0.4042 ± 0.13', 'dprs': '0.6802 ± 0.20', 'f1iou': 0.1586}\n",
      "{'method': 'skinny_st', 'train': 'medium', 'test': 'light', 'f1': '0.8904 ± 0.14', 'iou': '0.8214 ± 0.16', 'dprs': '0.1692 ± 0.18', 'f1iou': 0.069}\n",
      "{'method': 'bayes_st', 'train': 'medium', 'test': 'light', 'f1': '0.7032 ± 0.14', 'iou': '0.5571 ± 0.14', 'dprs': '0.5376 ± 0.23', 'f1iou': 0.1461}\n",
      "{'method': 'skinny_st', 'train': 'light', 'test': 'dark', 'f1': '0.7660 ± 0.17', 'iou': '0.6496 ± 0.21', 'dprs': '0.3402 ± 0.21', 'f1iou': 0.1164}\n",
      "{'method': 'bayes_st', 'train': 'light', 'test': 'dark', 'f1': '0.5293 ± 0.20', 'iou': '0.3852 ± 0.19', 'dprs': '0.6361 ± 0.22', 'f1iou': 0.1441}\n",
      "{'method': 'skinny_st', 'train': 'light', 'test': 'medium', 'f1': '0.9229 ± 0.11', 'iou': '0.8705 ± 0.13', 'dprs': '0.1192 ± 0.16', 'f1iou': 0.0524}\n",
      "{'method': 'bayes_st', 'train': 'light', 'test': 'medium', 'f1': '0.7853 ± 0.11', 'iou': '0.6574 ± 0.12', 'dprs': '0.3199 ± 0.16', 'f1iou': 0.1279}\n",
      "newdata:\n",
      "[{'method': 'skinny_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{0.7300 ± 0.25}', 'iou': '\\\\texttt{0.6279 ± 0.27}', 'dprs': '\\\\texttt{0.3805 ± 0.33}', 'f1iou': '\\\\texttt{\\\\textbf{0.1021}}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{0.7300 ± 0.25}', 'iou': '\\\\texttt{0.6279 ± 0.27}', 'dprs': '\\\\texttt{0.3805 ± 0.33}', 'f1iou': '\\\\texttt{\\\\textbf{0.1021}}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{0.7300 ± 0.25}', 'iou': '\\\\texttt{0.6279 ± 0.27}', 'dprs': '\\\\texttt{0.3805 ± 0.33}', 'f1iou': '\\\\texttt{\\\\textbf{0.1021}}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{0.7300 ± 0.25}', 'iou': '\\\\texttt{0.6279 ± 0.27}', 'dprs': '\\\\texttt{0.3805 ± 0.33}', 'f1iou': '\\\\texttt{\\\\textbf{0.1021}}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.7928 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.6668 ± 0.11}}', 'dprs': '\\\\texttt{\\\\textbf{0.3481 ± 0.16}}', 'f1iou': '\\\\texttt{0.1260}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.7928 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.6668 ± 0.11}}', 'dprs': '\\\\texttt{\\\\textbf{0.3481 ± 0.16}}', 'f1iou': '\\\\texttt{0.1260}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.7928 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.6668 ± 0.11}}', 'dprs': '\\\\texttt{\\\\textbf{0.3481 ± 0.16}}', 'f1iou': '\\\\texttt{0.1260}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.7928 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.6668 ± 0.11}}', 'dprs': '\\\\texttt{\\\\textbf{0.3481 ± 0.16}}', 'f1iou': '\\\\texttt{0.1260}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{0.7262 ± 0.26}', 'iou': '\\\\texttt{\\\\textbf{0.6276 ± 0.28}}', 'dprs': '\\\\texttt{\\\\textbf{0.3934 ± 0.34}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0986}}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{0.7262 ± 0.26}', 'iou': '\\\\texttt{\\\\textbf{0.6276 ± 0.28}}', 'dprs': '\\\\texttt{\\\\textbf{0.3934 ± 0.34}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0986}}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{0.7262 ± 0.26}', 'iou': '\\\\texttt{\\\\textbf{0.6276 ± 0.28}}', 'dprs': '\\\\texttt{\\\\textbf{0.3934 ± 0.34}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0986}}'}, {'method': 'skinny_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{0.7262 ± 0.26}', 'iou': '\\\\texttt{\\\\textbf{0.6276 ± 0.28}}', 'dprs': '\\\\texttt{\\\\textbf{0.3934 ± 0.34}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0986}}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.7577 ± 0.12}}', 'iou': '\\\\texttt{0.6229 ± 0.13}', 'dprs': '\\\\texttt{0.4679 ± 0.18}', 'f1iou': '\\\\texttt{0.1348}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.7577 ± 0.12}}', 'iou': '\\\\texttt{0.6229 ± 0.13}', 'dprs': '\\\\texttt{0.4679 ± 0.18}', 'f1iou': '\\\\texttt{0.1348}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.7577 ± 0.12}}', 'iou': '\\\\texttt{0.6229 ± 0.13}', 'dprs': '\\\\texttt{0.4679 ± 0.18}', 'f1iou': '\\\\texttt{0.1348}'}, {'method': 'bayes_st', 'train': 'dark', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.7577 ± 0.12}}', 'iou': '\\\\texttt{0.6229 ± 0.13}', 'dprs': '\\\\texttt{0.4679 ± 0.18}', 'f1iou': '\\\\texttt{0.1348}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.8447 ± 0.13}}', 'iou': '\\\\texttt{\\\\textbf{0.7486 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.2326 ± 0.17}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0961}}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.8447 ± 0.13}}', 'iou': '\\\\texttt{\\\\textbf{0.7486 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.2326 ± 0.17}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0961}}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.8447 ± 0.13}}', 'iou': '\\\\texttt{\\\\textbf{0.7486 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.2326 ± 0.17}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0961}}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.8447 ± 0.13}}', 'iou': '\\\\texttt{\\\\textbf{0.7486 ± 0.15}}', 'dprs': '\\\\texttt{\\\\textbf{0.2326 ± 0.17}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0961}}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{0.5628 ± 0.14}', 'iou': '\\\\texttt{0.4042 ± 0.13}', 'dprs': '\\\\texttt{0.6802 ± 0.20}', 'f1iou': '\\\\texttt{0.1586}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{0.5628 ± 0.14}', 'iou': '\\\\texttt{0.4042 ± 0.13}', 'dprs': '\\\\texttt{0.6802 ± 0.20}', 'f1iou': '\\\\texttt{0.1586}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{0.5628 ± 0.14}', 'iou': '\\\\texttt{0.4042 ± 0.13}', 'dprs': '\\\\texttt{0.6802 ± 0.20}', 'f1iou': '\\\\texttt{0.1586}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'dark', 'f1': '\\\\texttt{0.5628 ± 0.14}', 'iou': '\\\\texttt{0.4042 ± 0.13}', 'dprs': '\\\\texttt{0.6802 ± 0.20}', 'f1iou': '\\\\texttt{0.1586}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.8904 ± 0.14}}', 'iou': '\\\\texttt{\\\\textbf{0.8214 ± 0.16}}', 'dprs': '\\\\texttt{\\\\textbf{0.1692 ± 0.18}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0690}}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.8904 ± 0.14}}', 'iou': '\\\\texttt{\\\\textbf{0.8214 ± 0.16}}', 'dprs': '\\\\texttt{\\\\textbf{0.1692 ± 0.18}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0690}}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.8904 ± 0.14}}', 'iou': '\\\\texttt{\\\\textbf{0.8214 ± 0.16}}', 'dprs': '\\\\texttt{\\\\textbf{0.1692 ± 0.18}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0690}}'}, {'method': 'skinny_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{\\\\textbf{0.8904 ± 0.14}}', 'iou': '\\\\texttt{\\\\textbf{0.8214 ± 0.16}}', 'dprs': '\\\\texttt{\\\\textbf{0.1692 ± 0.18}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0690}}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{0.7032 ± 0.14}', 'iou': '\\\\texttt{0.5571 ± 0.14}', 'dprs': '\\\\texttt{0.5376 ± 0.23}', 'f1iou': '\\\\texttt{0.1461}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{0.7032 ± 0.14}', 'iou': '\\\\texttt{0.5571 ± 0.14}', 'dprs': '\\\\texttt{0.5376 ± 0.23}', 'f1iou': '\\\\texttt{0.1461}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{0.7032 ± 0.14}', 'iou': '\\\\texttt{0.5571 ± 0.14}', 'dprs': '\\\\texttt{0.5376 ± 0.23}', 'f1iou': '\\\\texttt{0.1461}'}, {'method': 'bayes_st', 'train': 'medium', 'test': 'light', 'f1': '\\\\texttt{0.7032 ± 0.14}', 'iou': '\\\\texttt{0.5571 ± 0.14}', 'dprs': '\\\\texttt{0.5376 ± 0.23}', 'f1iou': '\\\\texttt{0.1461}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.7660 ± 0.17}}', 'iou': '\\\\texttt{\\\\textbf{0.6496 ± 0.21}}', 'dprs': '\\\\texttt{\\\\textbf{0.3402 ± 0.21}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1164}}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.7660 ± 0.17}}', 'iou': '\\\\texttt{\\\\textbf{0.6496 ± 0.21}}', 'dprs': '\\\\texttt{\\\\textbf{0.3402 ± 0.21}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1164}}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.7660 ± 0.17}}', 'iou': '\\\\texttt{\\\\textbf{0.6496 ± 0.21}}', 'dprs': '\\\\texttt{\\\\textbf{0.3402 ± 0.21}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1164}}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{\\\\textbf{0.7660 ± 0.17}}', 'iou': '\\\\texttt{\\\\textbf{0.6496 ± 0.21}}', 'dprs': '\\\\texttt{\\\\textbf{0.3402 ± 0.21}}', 'f1iou': '\\\\texttt{\\\\textbf{0.1164}}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{0.5293 ± 0.20}', 'iou': '\\\\texttt{0.3852 ± 0.19}', 'dprs': '\\\\texttt{0.6361 ± 0.22}', 'f1iou': '\\\\texttt{0.1441}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{0.5293 ± 0.20}', 'iou': '\\\\texttt{0.3852 ± 0.19}', 'dprs': '\\\\texttt{0.6361 ± 0.22}', 'f1iou': '\\\\texttt{0.1441}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{0.5293 ± 0.20}', 'iou': '\\\\texttt{0.3852 ± 0.19}', 'dprs': '\\\\texttt{0.6361 ± 0.22}', 'f1iou': '\\\\texttt{0.1441}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'dark', 'f1': '\\\\texttt{0.5293 ± 0.20}', 'iou': '\\\\texttt{0.3852 ± 0.19}', 'dprs': '\\\\texttt{0.6361 ± 0.22}', 'f1iou': '\\\\texttt{0.1441}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.9229 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8705 ± 0.13}}', 'dprs': '\\\\texttt{\\\\textbf{0.1192 ± 0.16}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0524}}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.9229 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8705 ± 0.13}}', 'dprs': '\\\\texttt{\\\\textbf{0.1192 ± 0.16}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0524}}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.9229 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8705 ± 0.13}}', 'dprs': '\\\\texttt{\\\\textbf{0.1192 ± 0.16}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0524}}'}, {'method': 'skinny_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{\\\\textbf{0.9229 ± 0.11}}', 'iou': '\\\\texttt{\\\\textbf{0.8705 ± 0.13}}', 'dprs': '\\\\texttt{\\\\textbf{0.1192 ± 0.16}}', 'f1iou': '\\\\texttt{\\\\textbf{0.0524}}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{0.7853 ± 0.11}', 'iou': '\\\\texttt{0.6574 ± 0.12}', 'dprs': '\\\\texttt{0.3199 ± 0.16}', 'f1iou': '\\\\texttt{0.1279}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{0.7853 ± 0.11}', 'iou': '\\\\texttt{0.6574 ± 0.12}', 'dprs': '\\\\texttt{0.3199 ± 0.16}', 'f1iou': '\\\\texttt{0.1279}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{0.7853 ± 0.11}', 'iou': '\\\\texttt{0.6574 ± 0.12}', 'dprs': '\\\\texttt{0.3199 ± 0.16}', 'f1iou': '\\\\texttt{0.1279}'}, {'method': 'bayes_st', 'train': 'light', 'test': 'medium', 'f1': '\\\\texttt{0.7853 ± 0.11}', 'iou': '\\\\texttt{0.6574 ± 0.12}', 'dprs': '\\\\texttt{0.3199 ± 0.16}', 'f1iou': '\\\\texttt{0.1279}'}]\n",
      "{'f1_skinny_st_dark_medium': '\\\\texttt{0.7300 ± 0.25}', 'iou_skinny_st_dark_medium': '\\\\texttt{0.6279 ± 0.27}', 'dprs_skinny_st_dark_medium': '\\\\texttt{0.3805 ± 0.33}', 'f1iou_skinny_st_dark_medium': '\\\\texttt{\\\\textbf{0.1021}}', 'f1_bayes_st_dark_medium': '\\\\texttt{\\\\textbf{0.7928 ± 0.11}}', 'iou_bayes_st_dark_medium': '\\\\texttt{\\\\textbf{0.6668 ± 0.11}}', 'dprs_bayes_st_dark_medium': '\\\\texttt{\\\\textbf{0.3481 ± 0.16}}', 'f1iou_bayes_st_dark_medium': '\\\\texttt{0.1260}', 'f1_skinny_st_dark_light': '\\\\texttt{0.7262 ± 0.26}', 'iou_skinny_st_dark_light': '\\\\texttt{\\\\textbf{0.6276 ± 0.28}}', 'dprs_skinny_st_dark_light': '\\\\texttt{\\\\textbf{0.3934 ± 0.34}}', 'f1iou_skinny_st_dark_light': '\\\\texttt{\\\\textbf{0.0986}}', 'f1_bayes_st_dark_light': '\\\\texttt{\\\\textbf{0.7577 ± 0.12}}', 'iou_bayes_st_dark_light': '\\\\texttt{0.6229 ± 0.13}', 'dprs_bayes_st_dark_light': '\\\\texttt{0.4679 ± 0.18}', 'f1iou_bayes_st_dark_light': '\\\\texttt{0.1348}', 'f1_skinny_st_medium_dark': '\\\\texttt{\\\\textbf{0.8447 ± 0.13}}', 'iou_skinny_st_medium_dark': '\\\\texttt{\\\\textbf{0.7486 ± 0.15}}', 'dprs_skinny_st_medium_dark': '\\\\texttt{\\\\textbf{0.2326 ± 0.17}}', 'f1iou_skinny_st_medium_dark': '\\\\texttt{\\\\textbf{0.0961}}', 'f1_bayes_st_medium_dark': '\\\\texttt{0.5628 ± 0.14}', 'iou_bayes_st_medium_dark': '\\\\texttt{0.4042 ± 0.13}', 'dprs_bayes_st_medium_dark': '\\\\texttt{0.6802 ± 0.20}', 'f1iou_bayes_st_medium_dark': '\\\\texttt{0.1586}', 'f1_skinny_st_medium_light': '\\\\texttt{\\\\textbf{0.8904 ± 0.14}}', 'iou_skinny_st_medium_light': '\\\\texttt{\\\\textbf{0.8214 ± 0.16}}', 'dprs_skinny_st_medium_light': '\\\\texttt{\\\\textbf{0.1692 ± 0.18}}', 'f1iou_skinny_st_medium_light': '\\\\texttt{\\\\textbf{0.0690}}', 'f1_bayes_st_medium_light': '\\\\texttt{0.7032 ± 0.14}', 'iou_bayes_st_medium_light': '\\\\texttt{0.5571 ± 0.14}', 'dprs_bayes_st_medium_light': '\\\\texttt{0.5376 ± 0.23}', 'f1iou_bayes_st_medium_light': '\\\\texttt{0.1461}', 'f1_skinny_st_light_dark': '\\\\texttt{\\\\textbf{0.7660 ± 0.17}}', 'iou_skinny_st_light_dark': '\\\\texttt{\\\\textbf{0.6496 ± 0.21}}', 'dprs_skinny_st_light_dark': '\\\\texttt{\\\\textbf{0.3402 ± 0.21}}', 'f1iou_skinny_st_light_dark': '\\\\texttt{\\\\textbf{0.1164}}', 'f1_bayes_st_light_dark': '\\\\texttt{0.5293 ± 0.20}', 'iou_bayes_st_light_dark': '\\\\texttt{0.3852 ± 0.19}', 'dprs_bayes_st_light_dark': '\\\\texttt{0.6361 ± 0.22}', 'f1iou_bayes_st_light_dark': '\\\\texttt{0.1441}', 'f1_skinny_st_light_medium': '\\\\texttt{\\\\textbf{0.9229 ± 0.11}}', 'iou_skinny_st_light_medium': '\\\\texttt{\\\\textbf{0.8705 ± 0.13}}', 'dprs_skinny_st_light_medium': '\\\\texttt{\\\\textbf{0.1192 ± 0.16}}', 'f1iou_skinny_st_light_medium': '\\\\texttt{\\\\textbf{0.0524}}', 'f1_bayes_st_light_medium': '\\\\texttt{0.7853 ± 0.11}', 'iou_bayes_st_light_medium': '\\\\texttt{0.6574 ± 0.12}', 'dprs_bayes_st_light_medium': '\\\\texttt{0.3199 ± 0.16}', 'f1iou_bayes_st_light_medium': '\\\\texttt{0.1279}'}\n",
      "\n",
      "    \\begin{tabular}{clcccccc}\n",
      "    \\toprule\n",
      "    \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Training}} \n",
      "    & \\multicolumn{2}{c}{\\head{DARK}} & \\multicolumn{2}{c}{\\head{MEDIUM}} & \\multicolumn{2}{c}{\\head{LIGHT}} \\\\\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{\\head{Testing}}  & \\multicolumn{1}{c}{\\head{MEDIUM}}  & \\multicolumn{1}{c}{\\head{LIGHT}}  & \\multicolumn{1}{c}{\\head{DARK}}  & \\multicolumn{1}{c}{\\head{LIGHT}}  & \\multicolumn{1}{c}{\\head{DARK}}  & \\multicolumn{1}{c}{\\head{MEDIUM}} \\\\\\midrule\\multirow{2}{*}{{F1 $\\uparrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{0.7300 ± 0.25} & \\texttt{0.7262 ± 0.26} & \\texttt{\\textbf{0.8447 ± 0.13}} & \\texttt{\\textbf{0.8904 ± 0.14}} & \\texttt{\\textbf{0.7660 ± 0.17}} & \\texttt{\\textbf{0.9229 ± 0.11}}\\\\& BAYES & \\texttt{\\textbf{0.7928 ± 0.11}} & \\texttt{\\textbf{0.7577 ± 0.12}} & \\texttt{0.5628 ± 0.14} & \\texttt{0.7032 ± 0.14} & \\texttt{0.5293 ± 0.20} & \\texttt{0.7853 ± 0.11}\\\\\\multirow{2}{*}{{IOU $\\uparrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{0.6279 ± 0.27} & \\texttt{\\textbf{0.6276 ± 0.28}} & \\texttt{\\textbf{0.7486 ± 0.15}} & \\texttt{\\textbf{0.8214 ± 0.16}} & \\texttt{\\textbf{0.6496 ± 0.21}} & \\texttt{\\textbf{0.8705 ± 0.13}}\\\\& BAYES & \\texttt{\\textbf{0.6668 ± 0.11}} & \\texttt{0.6229 ± 0.13} & \\texttt{0.4042 ± 0.13} & \\texttt{0.5571 ± 0.14} & \\texttt{0.3852 ± 0.19} & \\texttt{0.6574 ± 0.12}\\\\\\multirow{2}{*}{{Dprs $\\downarrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{0.3805 ± 0.33} & \\texttt{\\textbf{0.3934 ± 0.34}} & \\texttt{\\textbf{0.2326 ± 0.17}} & \\texttt{\\textbf{0.1692 ± 0.18}} & \\texttt{\\textbf{0.3402 ± 0.21}} & \\texttt{\\textbf{0.1192 ± 0.16}}\\\\& BAYES & \\texttt{\\textbf{0.3481 ± 0.16}} & \\texttt{0.4679 ± 0.18} & \\texttt{0.6802 ± 0.20} & \\texttt{0.5376 ± 0.23} & \\texttt{0.6361 ± 0.22} & \\texttt{0.3199 ± 0.16}\\\\\\multirow{2}{*}{{F1 - IOU $\\downarrow$}}& SKINNY\\rule{0pt}{14pt} & \\texttt{\\textbf{0.1021}} & \\texttt{\\textbf{0.0986}} & \\texttt{\\textbf{0.0961}} & \\texttt{\\textbf{0.0690}} & \\texttt{\\textbf{0.1164}} & \\texttt{\\textbf{0.0524}}\\\\& BAYES & \\texttt{0.1260} & \\texttt{0.1348} & \\texttt{0.1586} & \\texttt{0.1461} & \\texttt{0.1441} & \\texttt{0.1279}\\\\\n",
      "    \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  method can be: base, cross\n",
    "method = 'cross'\n",
    "# mode can be: normal, skintones\n",
    "mode = 'skintones'\n",
    "\n",
    "if mode == 'normal':\n",
    "    db_list = ['ecu', 'hgr', 'schmugge']\n",
    "elif mode == 'skintones':\n",
    "    db_list = ['dark', 'medium', 'light']\n",
    "\n",
    "# paths resolving\n",
    "if method == 'base':\n",
    "    #db_list = ['ecu', 'hgr', 'schmugge']\n",
    "    detectors = ['skinny', 'bayes', 'dyc']\n",
    "    #detectors = ['skinny', 'bayes']\n",
    "    db_paths = []\n",
    "    for db in db_list:\n",
    "        for sd in detectors:\n",
    "            if db == 'hgr' and sd == 'dyc':\n",
    "                db = 'hgr_small'\n",
    "            if mode == 'skintones':\n",
    "                sd += '_st'\n",
    "            db_paths.append(f'dataset/{sd}/base/{db}')\n",
    "elif method == 'cross':\n",
    "    #db_list = ['ecu', 'hgr', 'schmugge']\n",
    "    detectors = ['skinny', 'bayes']\n",
    "    db_skinny = []\n",
    "    db_bayes = []\n",
    "    db_paths = []\n",
    "    for db_tr in db_list:\n",
    "        for db_te in db_list:\n",
    "            if db_te != db_tr:\n",
    "                for sd in detectors:\n",
    "                    if mode == 'skintones':\n",
    "                        sd += '_st'\n",
    "                    db_paths.append(f'dataset/{sd}/cross/{db_tr}_on_{db_te}')\n",
    "\n",
    "\n",
    "\n",
    "metrics = [f1, iou, dprs]\n",
    "json_table = []\n",
    "\n",
    "# compute metrics\n",
    "for ds in db_paths:\n",
    "    y_path = os.path.join(ds, 'y') # '{dataset}/y'\n",
    "    p_path = os.path.join(ds, 'p') # '{dataset}/p'\n",
    "    \n",
    "    rpd = pd_metrics_old(y_path, p_path, metrics)\n",
    "    # 'dataset/skinny/...'\n",
    "    skin_detector = ds.split('/')[1]\n",
    "\n",
    "    if method == 'base':\n",
    "        ds = ds + '_on_' + os.path.basename(ds)\n",
    "\n",
    "    table_item = print_pd_mean_old(rpd, metrics, desc=ds, method=skin_detector)\n",
    "\n",
    "    json_table.append(table_item)\n",
    "\n",
    "if method == 'base':\n",
    "    print(get_latex_base(json_table, db_list))\n",
    "else:\n",
    "    print(get_latex_cross(json_table, db_list))\n",
    "\n",
    "# save JSON table\n",
    "# out_table = open(\"metrics.json\", \"w\")\n",
    "# json.dump(json_table, out_table)\n",
    "# out_table.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gj2MY2z3V7q1",
    "outputId": "36c1e3ac-8fcf-46bf-e072-7779e9755b2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:15<00:00, 131.46it/s]\n",
      "  7%|▋         | 16/234 [00:00<00:01, 158.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 2000 matches\n",
      "ecu_on_ecu\n",
      "f1: 0.9133 ± 0.08\n",
      "iou: 0.8489 ± 0.12\n",
      "dprs: 0.1333 ± 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:01<00:00, 146.55it/s]\n",
      " 47%|████▋     | 59/126 [00:00<00:00, 589.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 234 matches\n",
      "ecu_on_hgr_small\n",
      "f1: 0.9284 ± 0.11\n",
      "iou: 0.8818 ± 0.15\n",
      "dprs: 0.1134 ± 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 619.15it/s]\n",
      "  1%|          | 13/2000 [00:00<00:16, 124.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 126 matches\n",
      "ecu_on_schmugge\n",
      "f1: 0.4862 ± 0.41\n",
      "iou: 0.4192 ± 0.37\n",
      "dprs: 0.7238 ± 0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:14<00:00, 133.60it/s]\n",
      "  7%|▋         | 16/234 [00:00<00:01, 150.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 2000 matches\n",
      "hgr_small_on_ecu\n",
      "f1: 0.7513 ± 0.19\n",
      "iou: 0.6339 ± 0.21\n",
      "dprs: 0.3588 ± 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:01<00:00, 150.99it/s]\n",
      " 44%|████▎     | 55/126 [00:00<00:00, 547.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 234 matches\n",
      "hgr_small_on_hgr_small\n",
      "f1: 0.9848 ± 0.02\n",
      "iou: 0.9705 ± 0.03\n",
      "dprs: 0.0251 ± 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 604.09it/s]\n",
      "  1%|          | 13/2000 [00:00<00:16, 123.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 126 matches\n",
      "hgr_small_on_schmugge\n",
      "f1: 0.2671 ± 0.31\n",
      "iou: 0.1969 ± 0.24\n",
      "dprs: 1.0009 ± 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:16<00:00, 122.16it/s]\n",
      "  7%|▋         | 16/234 [00:00<00:01, 150.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 2000 matches\n",
      "Schmugge_on_ecu\n",
      "f1: 0.6337 ± 0.21\n",
      "iou: 0.4963 ± 0.22\n",
      "dprs: 0.5336 ± 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:01<00:00, 138.14it/s]\n",
      " 44%|████▎     | 55/126 [00:00<00:00, 543.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 234 matches\n",
      "Schmugge_on_hgr_small\n",
      "f1: 0.7879 ± 0.21\n",
      "iou: 0.6933 ± 0.25\n",
      "dprs: 0.3185 ± 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 588.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 126 matches\n",
      "Schmugge_on_schmugge\n",
      "f1: 0.6121 ± 0.45\n",
      "iou: 0.585 ± 0.44\n",
      "dprs: 0.552 ± 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# paths resolving\n",
    "# method can be: skinny_ecu, dyc\n",
    "if method == 'skinny_ecu':\n",
    "#    data_dir = 'dataset/20210428-225749'\n",
    "#    datasets = ['ecu_on_abd-skin', 'ecu_on_ecu', 'ecu_on_hgr_small', 'ecu_on_pratheepan',\n",
    "#                'ecu_on_uchile', 'ecu_on_vdm']\n",
    "    data_dir = 'dataset/20210513-203631'\n",
    "    datasets = ['ecu_on_ecu', 'ecu_on_hgr_small', 'ecu_on_schmugge']\n",
    "elif method == 'skinny_hgr':\n",
    "    data_dir = 'dataset/20210513-190432'\n",
    "    datasets = ['hgr_small_on_ecu', 'hgr_small_on_hgr_small', 'hgr_small_on_schmugge']\n",
    "elif method == 'skinny_sch':\n",
    "    data_dir = 'dataset/20210512-205005'\n",
    "    datasets = ['Schmugge_on_ecu', 'Schmugge_on_hgr_small', 'Schmugge_on_schmugge']\n",
    "elif method == 'skinny':\n",
    "    data_dir = 'dataset/skinny'\n",
    "    datasets = ['ecu_on_ecu', 'ecu_on_hgr_small', 'ecu_on_schmugge',\n",
    "                'hgr_small_on_ecu', 'hgr_small_on_hgr_small', 'hgr_small_on_schmugge',\n",
    "                'Schmugge_on_ecu', 'Schmugge_on_hgr_small', 'Schmugge_on_schmugge']\n",
    "elif method == 'skinny_st':\n",
    "    data_dir = 'dataset/skinnyst'\n",
    "    datasets = ['dark_on_dark', 'dark_on_medium', 'dark_on_light',\n",
    "                'medium_on_dark', 'medium_on_medium', 'medium_on_light',\n",
    "                'light_on_dark', 'light_on_medium', 'light_on_light']\n",
    "elif method == 'dyc':\n",
    "    data_dir = 'dataset/predicted'\n",
    "    #datasets = ['abd-skin', 'ECU', 'HGR_small', 'HGR_big',\n",
    "    #            'Pratheepan', 'Uchile', 'Schmugge', 'VDM', 'VDM_test']\n",
    "    datasets = ['ecu', 'hgr_small', 'schmugge']\n",
    "# elif method == 'bayes':\n",
    "#     data_dir = 'dataset'\n",
    "#     datasets = ['ECU_on_ECU', 'ECU_on_HGR_small', 'ECU_on_Schmugge',\n",
    "#                 'HGR_small_on_ECU', 'HGR_small_on_HGR_small', 'HGR_small_on_Schmugge',\n",
    "#                 'Schmugge_on_ECU', 'Schmugge_on_HGR_small', 'Schmugge_on_Schmugge']\n",
    "elif method == 'bayes':\n",
    "    data_dir = 'dataset/bayesst'\n",
    "    datasets = ['ECU_on_ECU', 'ECU_on_HGR_small', 'ECU_on_Schmugge',\n",
    "                'HGR_small_on_ECU', 'HGR_small_on_HGR_small', 'HGR_small_on_Schmugge',\n",
    "                'Schmugge_on_ECU', 'Schmugge_on_HGR_small', 'Schmugge_on_Schmugge']\n",
    "elif method == 'bayes_st':\n",
    "    data_dir = 'dataset/bayesst'\n",
    "    datasets = ['dark_on_dark', 'dark_on_medium', 'dark_on_light',\n",
    "                'medium_on_dark', 'medium_on_medium', 'medium_on_light',\n",
    "                'light_on_dark', 'light_on_medium', 'light_on_light']\n",
    "\n",
    "\n",
    "#metrics = [mcc, iou, f1, f2, dprs, recall, precision, specificity]\n",
    "#metrics = [f1_n, dprs_n, recall, precision, specificity]\n",
    "metrics = [f1, iou, dprs]\n",
    "json_table = []\n",
    "\n",
    "# compute metrics\n",
    "for ds in datasets:\n",
    "    if method != 'skinny_sch' and method != 'bayes' and method != 'skinny':\n",
    "        ds = ds.lower() # in case it wasn't lowercase\n",
    "\n",
    "    #y_path = 'dataset/20210428-225749/ecu_on_vdm/y' # 'dataset/y'\n",
    "    #p_path = 'dataset/20210428-225749/ecu_on_vdm/p' # 'dataset/pred'\n",
    "\n",
    "    y_path = os.path.join(data_dir, ds, 'y') # 'dataset/y'\n",
    "    p_path = os.path.join(data_dir, ds, 'p') # 'dataset/pred'\n",
    "    \n",
    "    rpd = pd_metrics_old(y_path, p_path, metrics)\n",
    "    table_item = print_pd_mean_old(rpd, metrics, desc=ds, method=method)\n",
    "    #rpd = pd_metrics(y_path, p_path, metrics)\n",
    "    #print_pd_mean(rpd, metrics, desc=ds)\n",
    "\n",
    "    json_table.append(table_item)\n",
    "\n",
    "# save JSON table\n",
    "out_table = open(\"metrics.json\", \"w\")\n",
    "json.dump(json_table, out_table)\n",
    "out_table.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb2LNfLv5Q9k"
   },
   "source": [
    "ADDITIONAL INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOUPqAInVcTV",
    "outputId": "ab3a8e27-cad3-44f0-83cc-fd32ba8d3664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Case A1: Positively imbalanced dataset\n",
      "f1: 0.95\n",
      "mcc: -0.03\n",
      "\n",
      "Use Case A2: Positively imbalanced dataset\n",
      "f1: 0.12\n",
      "mcc: -0.24\n",
      "\n",
      "Use Case B1: Balanced dataset\n",
      "f1: 0.66\n",
      "mcc: 0.07\n",
      "\n",
      "Use Case B2: Balanced dataset\n",
      "f1: 0.31\n",
      "mcc: 0.17\n",
      "\n",
      "Use Case C1: Negatively imbalanced dataset\n",
      "f1: 0.17\n",
      "mcc: -0.19\n",
      "\n",
      "Use Case C2: Negatively imbalanced dataset\n",
      "f1: 0.29\n",
      "mcc: 0.31\n"
     ]
    }
   ],
   "source": [
    "# test MCC and F1 correctness based on paper data\n",
    "\n",
    "\n",
    "# Summary: F1 doesn't care much about TN and could signal \n",
    "#          over-optimistic data to the classifier\n",
    "\n",
    "\n",
    "print('Use Case A1: Positively imbalanced dataset')\n",
    "data = {}\n",
    "data['ap'] = 91   # 91 sick patients\n",
    "data['an'] = 9    # 9 healthy individuals\n",
    "data['se'] = 99\n",
    "data['tp'] = 90   # algorithm is good at predicting positive data\n",
    "data['fp'] = 9\n",
    "data['tn'] = 0\n",
    "data['fn'] = 1    # algorithm is bad at predicting negative data\n",
    "# F1 measures an almost perfect score, MCC instead measures a bad score\n",
    "# F1 0.95    MCC -0.03\n",
    "print(f'f1: {round(f1(data), 2)}\\nmcc: {round(mcc(data), 2)}')\n",
    "\n",
    "\n",
    "print('\\nUse Case A2: Positively imbalanced dataset')\n",
    "data = {}\n",
    "data['ap'] = 75   # 75 positives\n",
    "data['an'] = 25   # 25 negatives\n",
    "data['se'] = 11\n",
    "data['tp'] = 5    # classifier unable to predict positives\n",
    "data['fp'] = 6\n",
    "data['tn'] = 19   # classifier was able to predict negatives\n",
    "data['fn'] = 70\n",
    "# In this case both the metrics measure a bad score\n",
    "# F1 0.12    MCC -0.24\n",
    "print(f'f1: {round(f1(data), 2)}\\nmcc: {round(mcc(data), 2)}')\n",
    "\n",
    "\n",
    "print('\\nUse Case B1: Balanced dataset')\n",
    "data = {}\n",
    "data['ap'] = 50   # 50 positives\n",
    "data['an'] = 50   # 50 negatives\n",
    "data['se'] = 92\n",
    "data['tp'] = 47   # classifier able to predict positives\n",
    "data['fp'] = 45\n",
    "data['tn'] = 5    # classifier was unable to predict negatives\n",
    "data['fn'] = 3\n",
    "# F1 measures a good score, MCC doesn't\n",
    "# F1 0.66    MCC 0.07\n",
    "print(f'f1: {round(f1(data), 2)}\\nmcc: {round(mcc(data), 2)}')\n",
    "\n",
    "\n",
    "print('\\nUse Case B2: Balanced dataset')\n",
    "data = {}\n",
    "data['ap'] = 50   # 50 positives\n",
    "data['an'] = 50   # 50 negatives\n",
    "data['se'] = 14\n",
    "data['tp'] = 10   # classifier was unable to predict positives\n",
    "data['fp'] = 4\n",
    "data['tn'] = 46    # classifier able to predict negatives\n",
    "data['fn'] = 40\n",
    "# F1 measures a good score, MCC doesn't\n",
    "# F1 0.31    MCC 0.17\n",
    "print(f'f1: {round(f1(data), 2)}\\nmcc: {round(mcc(data), 2)}')\n",
    "\n",
    "\n",
    "print('\\nUse Case C1: Negatively imbalanced dataset')\n",
    "data = {}\n",
    "data['ap'] = 10   # 10 positives\n",
    "data['an'] = 90   # 90 negatives\n",
    "data['se'] = 98\n",
    "data['tp'] = 9    # classifier was unable to predict positives\n",
    "data['fp'] = 89\n",
    "data['tn'] = 1    # classifier able to predict negatives\n",
    "data['fn'] = 1\n",
    "# Both the scores gives bad measure\n",
    "# F1 0.17    MCC -0.19\n",
    "print(f'f1: {round(f1(data), 2)}\\nmcc: {round(mcc(data), 2)}')\n",
    "\n",
    "\n",
    "print('\\nUse Case C2: Negatively imbalanced dataset')\n",
    "data = {}\n",
    "data['ap'] = 11   # 10 positives\n",
    "data['an'] = 89   # 89 negatives\n",
    "data['se'] = 3\n",
    "data['tp'] = 2   # classifier was unable to predict positives\n",
    "data['fp'] = 1\n",
    "data['tn'] = 88    # classifier able to predict negatives\n",
    "data['fn'] = 9\n",
    "# Both the scores gives bad measure\n",
    "# F1 0.29    MCC 0.31\n",
    "print(f'f1: {round(f1(data), 2)}\\nmcc: {round(mcc(data), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vj71I_5QK0E8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "a_file = open(\"data.json\", \"w\")\n",
    "json.dump(reso, a_file)\n",
    "a_file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bench.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
